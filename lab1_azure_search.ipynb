{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from approaches.azureretriver import AzureRetrieveApproach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctags_path = './repo/langchain/libs/langchain/tags'\n",
    "ctags_root_path = os.path.dirname(ctags_path)\n",
    "assert os.path.isfile(ctags_path), \"Please run `zsh download_example_rpo.sh` first\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_SEARCH_SERVICE = os.environ[\"AZURE_SEARCH_SERVICE\"]\n",
    "AZURE_SEARCH_TINY_INDEX = os.environ[\"AZURE_SEARCH_TINY_INDEX\"]\n",
    "AZURE_SEARCH_BIGGER_INDEX = os.environ[\"AZURE_SEARCH_BIGGER_INDEX\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tags_file(file_path: str) -> list[dict]:\n",
    "    with open(file_path, 'r', errors='ignore') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    tags = []\n",
    "    for line in lines:\n",
    "        if line.startswith('!'):  # Skip metadata lines\n",
    "            continue\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) >= 4:\n",
    "            tag_name = parts[0]\n",
    "            file_name = parts[1]\n",
    "            pattern = parts[2]\n",
    "            tags.append(dict(tag_name=tag_name, file_name=file_name, pattern=pattern))\n",
    "\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/aicontest/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\") # vector length 384\n",
    "embedding_dimension = embeddings.client.get_sentence_embedding_dimension()\n",
    "\n",
    "def get_embeddings(text: str, normalize=True) -> list:\n",
    "    embeddings.encode_kwargs = {'normalize_embeddings': normalize}\n",
    "    return embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import importlib.util\n",
    "import os\n",
    "\n",
    "def get_source_code(function_name, function_path, ctags_root_path=ctags_root_path):\n",
    "    spec=importlib.util.spec_from_file_location(function_name, os.path.join(ctags_root_path, function_path))\n",
    "    foo = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(foo)\n",
    "    return inspect.getsource(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_code_file_text(metadatas : list[dict]):\n",
    "    code_file_text = \"\"\n",
    "    for i, metadata in enumerate(metadatas):\n",
    "        code_file_text += f'==== File {i+1}/{len(metadata)} ====\\n'\n",
    "        code_file_text += f'File path: {metadata[\"file_name\"]}\\n'\n",
    "        code_file_text += f'Tag name: {metadata[\"tag_name\"]}\\n'\n",
    "        code_string = get_source_code(metadata[\"tag_name\"], metadata[\"file_name\"])\n",
    "        code_file_text += f'Code: {code_string}\\n'\n",
    "        code_file_text += \"\\n\"\n",
    "    return code_file_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a hupful bot that fuilfill the human' program task:\n",
    "\n",
    "The following is releative code:\n",
    "{code_file_text}\n",
    "\n",
    "User: {user_prompt}\n",
    "Ai:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Azure client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_retriever = AzureRetrieveApproach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azure_retriever.create_index(indedx_name=\"poc_20231202\", embedding_dimension=embedding_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read ctag, embedding and upload to Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctags_root_path = os.path.dirname(ctags_path)\n",
    "# tags = read_tags_file(ctags_path)\n",
    "\n",
    "# print(f\"Total tags: {len(tags)}\")\n",
    "\n",
    "# tags = tags[:5000]\n",
    "\n",
    "# documents = []\n",
    "# idx = 0\n",
    "# for tag in tqdm(tags):\n",
    "#     documents.append(\n",
    "#         dict(\n",
    "#             id=str(idx),\n",
    "#             title=tag['file_name'],\n",
    "#             metadata=json.dumps(tag),\n",
    "#             content=f\"{tag['file_name']} | {tag['tag_name']}\",\n",
    "#             category=\"code\",\n",
    "#             titleVector=get_embeddings(f\"{tag['file_name']} | {tag['tag_name']}\"),\n",
    "#             contentVector=get_embeddings(tag['tag_name'])\n",
    "#         )\n",
    "#     )\n",
    "#     idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azure_retriever.batch_update(documents=documents, index_name=\"poc_20231202\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search using vector similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"How can i add a Custom Prompt Template in this repository? also add the unit-test. Give me an example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = azure_retriever.search(\n",
    "    index_name=\"poc_20231202\", \n",
    "    vector=get_embeddings(text), \n",
    "    fields=\"contentVector\", \n",
    "    top=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: langchain/chains/query_constructor/prompt.py\n",
      "Score: 0.7941357\n",
      "Metadata: {\"tag_name\": \"EXAMPLE_PROMPT_TEMPLATE\", \"file_name\": \"langchain/chains/query_constructor/prompt.py\", \"pattern\": \"/^EXAMPLE_PROMPT_TEMPLATE = \\\"\\\"\\\"\\\\\\\\$/;\\\"\"}\n",
      "Content: langchain/chains/query_constructor/prompt.py | EXAMPLE_PROMPT_TEMPLATE\n",
      "Category: code\n",
      "\n",
      "Title: langchain/chains/natbot/prompt.py\n",
      "Score: 0.7826364\n",
      "Metadata: {\"tag_name\": \"_PROMPT_TEMPLATE\", \"file_name\": \"langchain/chains/natbot/prompt.py\", \"pattern\": \"/^_PROMPT_TEMPLATE = \\\"\\\"\\\"$/;\\\"\"}\n",
      "Content: langchain/chains/natbot/prompt.py | _PROMPT_TEMPLATE\n",
      "Category: code\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Metadata: {result['metadata']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search using hybrid (text keyword and vector similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = azure_retriever.hybrid_search(\n",
    "    index_name=\"poc_20231202\", \n",
    "    text=text,\n",
    "    vector=get_embeddings(text), \n",
    "    fields=\"contentVector\", \n",
    "    top=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: langchain/chains/query_constructor/prompt.py\n",
      "Score: 0.01666666753590107\n",
      "Metadata: {\"tag_name\": \"EXAMPLE_PROMPT_TEMPLATE\", \"file_name\": \"langchain/chains/query_constructor/prompt.py\", \"pattern\": \"/^EXAMPLE_PROMPT_TEMPLATE = \\\"\\\"\\\"\\\\\\\\$/;\\\"\"}\n",
      "Content: langchain/chains/query_constructor/prompt.py | EXAMPLE_PROMPT_TEMPLATE\n",
      "Category: code\n",
      "\n",
      "Title: tests/integration_tests/examples/example-utf8.html\n",
      "Score: 0.01666666753590107\n",
      "Metadata: {\"tag_name\": \"Chase the red dot\", \"file_name\": \"tests/integration_tests/examples/example-utf8.html\", \"pattern\": \"/^    <h2>Chase the red dot<\\\\/h2>$/;\\\"\"}\n",
      "Content: tests/integration_tests/examples/example-utf8.html | Chase the red dot\n",
      "Category: code\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Metadata: {result['metadata']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search using hybrid_reranking_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = azure_retriever.hybrid_reranking_search(\n",
    "    index_name=\"poc_20231202\", \n",
    "    text=text,\n",
    "    vector=get_embeddings(text), \n",
    "    fields=\"contentVector\", \n",
    "    top=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: langchain/chains/flare/prompts.py\n",
      "Score: 0.0117647061124444\n",
      "Metadata: {\"tag_name\": \"PROMPT\", \"file_name\": \"langchain/chains/flare/prompts.py\", \"pattern\": \"/^PROMPT = PromptTemplate($/;\\\"\"}\n",
      "Content: langchain/chains/flare/prompts.py | PROMPT\n",
      "Category: code\n",
      "\n",
      "Title: langchain/chains/summarize/refine_prompts.py\n",
      "Score: 0.012048192322254181\n",
      "Metadata: {\"tag_name\": \"PROMPT\", \"file_name\": \"langchain/chains/summarize/refine_prompts.py\", \"pattern\": \"/^PROMPT = PromptTemplate.from_template(prompt_template)$/;\\\"\"}\n",
      "Content: langchain/chains/summarize/refine_prompts.py | PROMPT\n",
      "Category: code\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Metadata: {result['metadata']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/aicontest/lib/python3.11/site-packages/langchain/chat_models/azure_openai.py:155: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://f19855e6-c488-4c48-a0f0-e7bb2b9527fa-canadaeast.openai.azure.com to https://f19855e6-c488-4c48-a0f0-e7bb2b9527fa-canadaeast.openai.azure.com/openai.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/aicontest/lib/python3.11/site-packages/langchain/chat_models/azure_openai.py:162: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/aicontest/lib/python3.11/site-packages/langchain/chat_models/azure_openai.py:170: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://f19855e6-c488-4c48-a0f0-e7bb2b9527fa-canadaeast.openai.azure.com to https://f19855e6-c488-4c48-a0f0-e7bb2b9527fa-canadaeast.openai.azure.com/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=os.environ.get(\"DEPLOYMENT_NAME\"),\n",
    "    temperature=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(user_prompt: str, retriever_type: str = \"vector_search\") -> str:\n",
    "    # find docs similar to user_prompt\n",
    "\n",
    "    if retriever_type == \"vector_search\":\n",
    "        results = azure_retriever.search(\n",
    "            index_name=\"poc_20231202\", \n",
    "            vector=get_embeddings(user_prompt), \n",
    "            fields=\"contentVector\", \n",
    "            top=10\n",
    "        )\n",
    "    elif retriever_type == \"hybrid_search\" or retriever_type == \"hybrid_reranking_search\":\n",
    "        results = azure_retriever.hybrid_reranking_search(\n",
    "            index_name=\"poc_20231202\", \n",
    "            text=user_prompt,\n",
    "            vector=get_embeddings(user_prompt), \n",
    "            fields=\"contentVector\", \n",
    "            top=10\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"retriever_type: {retriever_type} is not supported\")\n",
    "\n",
    "    metadatas = []\n",
    "    for result in results:\n",
    "        result_dict = json.loads(result['metadata'])\n",
    "        if result_dict['file_name'].endswith(\".py\"):\n",
    "            metadatas.append(result_dict)\n",
    "    metadatas = metadatas[:3]\n",
    "\n",
    "    citations = [metadata[\"file_name\"] for metadata in metadatas]\n",
    "\n",
    "    user_prompt = template.format(code_file_text=create_code_file_text(metadatas), user_prompt=user_prompt)\n",
    "    \n",
    "    # call openai api here\n",
    "    message = HumanMessage(content=user_prompt)\n",
    "    final_message = llm([message]).content  \n",
    "\n",
    "    citations_str = \"\\n\".join(citations)\n",
    "    final_message += f\"\\n\\nCitations:\\n{citations_str}\"\n",
    "\n",
    "    return final_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "👩‍💻 : What is Langchain design for?\n",
      "====================\n",
      "🤖 : Langchain is designed to facilitate the creation and execution of conversational AI systems. It provides a framework for building chains of components that can process and transform input, carry on conversations, and generate responses. The design allows for flexibility in incorporating different language models and memory systems, enabling the development of sophisticated conversational agents.\n",
      "\n",
      "Citations:\n",
      "langchain/chains/transform.py\n",
      "langchain/vectorstores/sklearn.py\n",
      "langchain/chains/conversation/base.py\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What is Langchain design for?\"\n",
    "result = ask(user_question, retriever_type=\"hybrid_reranking_search\")\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(f\"👩‍💻 : {user_question}\")\n",
    "print(\"=\"*20)\n",
    "print(f\"🤖 : {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "👩‍💻 : How can i add a Custom Prompt Template in this repository? also add the unit-test. Give me an example\n",
      "====================\n",
      "🤖 : To add a custom prompt template in this repository, you can follow these steps:\n",
      "\n",
      "1. Create a new Python file in the appropriate directory (e.g., `langchain/chains/custom/prompts.py`).\n",
      "2. Define your custom prompt template using the `PromptTemplate` class from `langchain_core.prompts`.\n",
      "3. Write your custom prompt code, including any necessary input variables and template strings.\n",
      "4. Add unit tests for your custom prompt template in the appropriate test file (e.g., `tests/chains/custom/test_prompts.py`).\n",
      "\n",
      "Here's an example of how you can add a custom prompt template:\n",
      "\n",
      "==== File 1/2 ====\n",
      "File path: langchain/chains/custom/prompts.py\n",
      "Tag name: PROMPT\n",
      "Code:\n",
      "```python\n",
      "from langchain_core.prompts import PromptTemplate\n",
      "\n",
      "CUSTOM_PROMPT_TEMPLATE = \"\"\"\\\n",
      "This is a custom prompt template.\n",
      "\n",
      "Input 1: {input1}\n",
      "Input 2: {input2}\n",
      "Input 3: {input3}\n",
      "\n",
      ">>> INPUT 1: {input1}\n",
      ">>> INPUT 2: {input2}\n",
      ">>> INPUT 3: {input3}\n",
      "\"\"\"\n",
      "\n",
      "CUSTOM_PROMPT = PromptTemplate(\n",
      "    template=CUSTOM_PROMPT_TEMPLATE,\n",
      "    input_variables=[\"input1\", \"input2\", \"input3\"],\n",
      ")\n",
      "```\n",
      "\n",
      "==== File 2/2 ====\n",
      "File path: tests/chains/custom/test_prompts.py\n",
      "Tag name: UNIT_TEST\n",
      "Code:\n",
      "```python\n",
      "from langchain_core.prompts import PromptTemplate\n",
      "from langchain.chains.custom.prompts import CUSTOM_PROMPT\n",
      "\n",
      "def test_custom_prompt_template():\n",
      "    assert isinstance(CUSTOM_PROMPT, PromptTemplate)\n",
      "    assert CUSTOM_PROMPT.template == \"\"\"\\\n",
      "This is a custom prompt template.\n",
      "\n",
      "Input 1: {input1}\n",
      "Input 2: {input2}\n",
      "Input 3: {input3}\n",
      "\n",
      ">>> INPUT 1: {input1}\n",
      ">>> INPUT 2: {input2}\n",
      ">>> INPUT 3: {input3}\n",
      "\"\"\"\n",
      "\n",
      "    assert CUSTOM_PROMPT.input_variables == [\"input1\", \"input2\", \"input3\"]\n",
      "```\n",
      "\n",
      "Make sure to replace `CUSTOM_PROMPT_TEMPLATE`, `CUSTOM_PROMPT`, and the file paths with your own custom names and paths.\n",
      "\n",
      "Please note that you should adjust the file paths and tag names based on your repository structure and conventions.\n",
      "\n",
      "Citations:\n",
      "langchain/chains/flare/prompts.py\n",
      "langchain/chains/summarize/refine_prompts.py\n",
      "langchain/chains/natbot/prompt.py\n"
     ]
    }
   ],
   "source": [
    "user_question = \"How can i add a Custom Prompt Template in this repository? also add the unit-test. Give me an example\"\n",
    "result = ask(user_question, retriever_type=\"hybrid_reranking_search\")\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(f\"👩‍💻 : {user_question}\")\n",
    "print(\"=\"*20)\n",
    "print(f\"🤖 : {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicontest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
