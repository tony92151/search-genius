{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from approaches.azureretriver import AzureRetrieveApproach\n",
    "from utils.tagreader import read_tags_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctags_path = './repo/langchain/libs/langchain/tags'\n",
    "ctags_root_path = os.path.dirname(ctags_path)\n",
    "assert os.path.isfile(ctags_path), \"Please run `zsh download_example_rpo.sh` first\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_SEARCH_SERVICE = os.environ[\"AZURE_SEARCH_SERVICE\"]\n",
    "AZURE_SEARCH_TINY_INDEX = os.environ[\"AZURE_SEARCH_TINY_INDEX\"]\n",
    "AZURE_SEARCH_BIGGER_INDEX = os.environ[\"AZURE_SEARCH_BIGGER_INDEX\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/aicontest/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\") # vector length 384\n",
    "embedding_dimension = embeddings.client.get_sentence_embedding_dimension()\n",
    "\n",
    "def get_embeddings(text: str, normalize=True) -> list:\n",
    "    embeddings.encode_kwargs = {'normalize_embeddings': normalize}\n",
    "    return embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import importlib.util\n",
    "import os\n",
    "\n",
    "def get_source_code(function_name, function_path, ctags_root_path=ctags_root_path):\n",
    "    spec=importlib.util.spec_from_file_location(function_name, os.path.join(ctags_root_path, function_path))\n",
    "    foo = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(foo)\n",
    "    return inspect.getsource(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_code_file_text(metadatas : list[dict]):\n",
    "    code_file_text = \"\"\n",
    "    for i, metadata in enumerate(metadatas):\n",
    "        code_file_text += f'==== File {i+1}/{len(metadata)} ====\\n'\n",
    "        code_file_text += f'File path: {metadata[\"file_name\"]}\\n'\n",
    "        code_file_text += f'Tag name: {metadata[\"tag_name\"]}\\n'\n",
    "        code_string = get_source_code(metadata[\"tag_name\"], metadata[\"file_name\"])\n",
    "        code_file_text += f'Code: {code_string}\\n'\n",
    "        code_file_text += \"\\n\"\n",
    "    return code_file_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a hupful bot that fuilfill the human' program task:\n",
    "\n",
    "The following is releative code:\n",
    "{code_file_text}\n",
    "\n",
    "User: {user_prompt}\n",
    "Ai:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Azure client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_retriever = AzureRetrieveApproach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poc_20231202 created\n"
     ]
    }
   ],
   "source": [
    "azure_retriever.create_index(indedx_name=\"poc_20231202\", embedding_dimension=embedding_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read ctag, embedding and upload to Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tags: 13400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13400/13400 [08:10<00:00, 27.32it/s]\n"
     ]
    }
   ],
   "source": [
    "ctags_root_path = os.path.dirname(ctags_path)\n",
    "tags = read_tags_file(ctags_path, accept_file=[\".py\"])\n",
    "\n",
    "print(f\"Total tags: {len(tags)}\")\n",
    "\n",
    "# tags = tags[:5000]\n",
    "\n",
    "documents = []\n",
    "idx = 0\n",
    "for tag in tqdm(tags):\n",
    "    documents.append(\n",
    "        dict(\n",
    "            id=str(idx),\n",
    "            title=tag['file_name'],\n",
    "            metadata=json.dumps(tag),\n",
    "            content=f\"{tag['file_name']} | {tag['tag_name']} | \",\n",
    "            category=\"code\",\n",
    "            titleVector=get_embeddings(f\"{tag['file_name']} | {tag['tag_name']}\"),\n",
    "            contentVector=get_embeddings(tag['tag_name'])\n",
    "        )\n",
    "    )\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_retriever.batch_update(documents=documents, index_name=\"poc_20231202\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search using vector similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"How can i add a Custom Prompt Template in this repository? also add the unit-test. Give me an example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = azure_retriever.search(\n",
    "    index_name=\"poc_20231202\", \n",
    "    vector=get_embeddings(text), \n",
    "    fields=\"contentVector\", \n",
    "    top=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: langchain/chains/summarize/refine_prompts.py\n",
      "Score: 0.768639\n",
      "Metadata: {\"tag_name\": \"prompt_template\", \"file_name\": \"langchain/chains/summarize/refine_prompts.py\", \"pattern\": \"/^prompt_template = \\\"\\\"\\\"Write a concise summary of the following:$/;\\\"\"}\n",
      "Content: langchain/chains/summarize/refine_prompts.py | prompt_template\n",
      "Category: code\n",
      "\n",
      "Title: langchain/retrievers/document_compressors/chain_extract_prompt.py\n",
      "Score: 0.768639\n",
      "Metadata: {\"tag_name\": \"prompt_template\", \"file_name\": \"langchain/retrievers/document_compressors/chain_extract_prompt.py\", \"pattern\": \"/^prompt_template = \\\"\\\"\\\"Given the following question and context, extract any part of the context */;\\\"\"}\n",
      "Content: langchain/retrievers/document_compressors/chain_extract_prompt.py | prompt_template\n",
      "Category: code\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Metadata: {result['metadata']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search using hybrid (text keyword and vector similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = azure_retriever.hybrid_search(\n",
    "    index_name=\"poc_20231202\", \n",
    "    text=text,\n",
    "    vector=get_embeddings(text), \n",
    "    fields=\"contentVector\", \n",
    "    top=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: langchain/chains/summarize/refine_prompts.py\n",
      "Score: 0.01666666753590107\n",
      "Metadata: {\"tag_name\": \"prompt_template\", \"file_name\": \"langchain/chains/summarize/refine_prompts.py\", \"pattern\": \"/^prompt_template = \\\"\\\"\\\"Write a concise summary of the following:$/;\\\"\"}\n",
      "Content: langchain/chains/summarize/refine_prompts.py | prompt_template\n",
      "Category: code\n",
      "\n",
      "Title: langchain/docstore/in_memory.py\n",
      "Score: 0.01666666753590107\n",
      "Metadata: {\"tag_name\": \"add\", \"file_name\": \"langchain/docstore/in_memory.py\", \"ex_command\": \"^    def add(self, texts: Dict[str, Document]) -> None:$\", \"tag_kind\": \"m\", \"extension_fields\": \"class:InMemoryDocstore\"}\n",
      "Content: langchain/docstore/in_memory.py | add | \n",
      "Category: code\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Metadata: {result['metadata']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search using hybrid_reranking_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = azure_retriever.hybrid_reranking_search(\n",
    "    index_name=\"poc_20231202\", \n",
    "    text=text,\n",
    "    vector=get_embeddings(text), \n",
    "    fields=\"contentVector\", \n",
    "    top=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: langchain/chains/summarize/refine_prompts.py\n",
      "Score: 0.01666666753590107\n",
      "Metadata: {\"tag_name\": \"prompt_template\", \"file_name\": \"langchain/chains/summarize/refine_prompts.py\", \"pattern\": \"/^prompt_template = \\\"\\\"\\\"Write a concise summary of the following:$/;\\\"\"}\n",
      "Content: langchain/chains/summarize/refine_prompts.py | prompt_template\n",
      "Category: code\n",
      "\n",
      "Caption: langchain/chains/summarize/refine_prompts.py. code. langchain/chains/summarize/refine_prompts.py | prompt_template.\n",
      "\n",
      "Title: langchain/chains/chat_vector_db/prompts.py\n",
      "Score: 0.016393441706895828\n",
      "Metadata: {\"tag_name\": \"prompt_template\", \"file_name\": \"langchain/chains/chat_vector_db/prompts.py\", \"pattern\": \"/^prompt_template = \\\"\\\"\\\"Use the following pieces of context to answer the question at the end. If y/;\\\"\"}\n",
      "Content: langchain/chains/chat_vector_db/prompts.py | prompt_template\n",
      "Category: code\n",
      "\n",
      "Caption: langchain/chains/chat_vector_db/prompts.py. code. langchain/chains/chat_vector_db/prompts.py |<em> prompt_template.</em>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Metadata: {result['metadata']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  \n",
    "\n",
    "    captions = result.get(\"@search.captions\")\n",
    "    if captions:\n",
    "        caption = captions[0]\n",
    "        if caption.highlights:\n",
    "            print(f\"Caption: {caption.highlights}\\n\")\n",
    "        else:\n",
    "            print(f\"Caption: {caption.text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/aicontest/lib/python3.11/site-packages/langchain/chat_models/azure_openai.py:155: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://f19855e6-c488-4c48-a0f0-e7bb2b9527fa-canadaeast.openai.azure.com to https://f19855e6-c488-4c48-a0f0-e7bb2b9527fa-canadaeast.openai.azure.com/openai.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/aicontest/lib/python3.11/site-packages/langchain/chat_models/azure_openai.py:162: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/aicontest/lib/python3.11/site-packages/langchain/chat_models/azure_openai.py:170: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://f19855e6-c488-4c48-a0f0-e7bb2b9527fa-canadaeast.openai.azure.com to https://f19855e6-c488-4c48-a0f0-e7bb2b9527fa-canadaeast.openai.azure.com/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=os.environ.get(\"DEPLOYMENT_NAME\"),\n",
    "    temperature=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(user_prompt: str, retriever_type: str = \"vector_search\") -> str:\n",
    "    # find docs similar to user_prompt\n",
    "\n",
    "    if retriever_type == \"vector_search\":\n",
    "        results = azure_retriever.search(\n",
    "            index_name=\"poc_20231202\", \n",
    "            vector=get_embeddings(user_prompt), \n",
    "            fields=\"contentVector\", \n",
    "            top=10\n",
    "        )\n",
    "    elif retriever_type == \"hybrid_search\" or retriever_type == \"hybrid_reranking_search\":\n",
    "        results = azure_retriever.hybrid_reranking_search(\n",
    "            index_name=\"poc_20231202\", \n",
    "            text=user_prompt,\n",
    "            vector=get_embeddings(user_prompt), \n",
    "            fields=\"contentVector\", \n",
    "            top=10\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"retriever_type: {retriever_type} is not supported\")\n",
    "\n",
    "    metadatas = []\n",
    "    for result in results:\n",
    "        result_dict = json.loads(result['metadata'])\n",
    "        if result_dict['file_name'].endswith(\".py\"):\n",
    "            metadatas.append(result_dict)\n",
    "    metadatas = metadatas[:3]\n",
    "\n",
    "    citations = [metadata[\"file_name\"] for metadata in metadatas]\n",
    "\n",
    "    user_prompt = template.format(code_file_text=create_code_file_text(metadatas), user_prompt=user_prompt)\n",
    "    \n",
    "    # call openai api here\n",
    "    message = HumanMessage(content=user_prompt)\n",
    "    final_message = llm([message]).content  \n",
    "\n",
    "    citations_str = \"\\n\".join(citations)\n",
    "    final_message += f\"\\n\\nCitations:\\n{citations_str}\"\n",
    "\n",
    "    return final_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '__dict__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb 儲存格 30\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m user_question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat is Langchain design for?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m result \u001b[39m=\u001b[39m ask(user_question, retriever_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhybrid_reranking_search\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m👩‍💻 : \u001b[39m\u001b[39m{\u001b[39;00muser_question\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb 儲存格 30\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb#X42sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m metadatas \u001b[39m=\u001b[39m metadatas[:\u001b[39m3\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb#X42sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m citations \u001b[39m=\u001b[39m [metadata[\u001b[39m\"\u001b[39m\u001b[39mfile_name\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m metadata \u001b[39min\u001b[39;00m metadatas]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb#X42sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m user_prompt \u001b[39m=\u001b[39m template\u001b[39m.\u001b[39mformat(code_file_text\u001b[39m=\u001b[39mcreate_code_file_text(metadatas), user_prompt\u001b[39m=\u001b[39muser_prompt)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb#X42sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# call openai api here\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb#X42sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m message \u001b[39m=\u001b[39m HumanMessage(content\u001b[39m=\u001b[39muser_prompt)\n",
      "\u001b[1;32m/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb 儲存格 30\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m code_file_text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFile path: \u001b[39m\u001b[39m{\u001b[39;00mmetadata[\u001b[39m\"\u001b[39m\u001b[39mfile_name\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb#X42sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m code_file_text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTag name: \u001b[39m\u001b[39m{\u001b[39;00mmetadata[\u001b[39m\"\u001b[39m\u001b[39mtag_name\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb#X42sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m code_string \u001b[39m=\u001b[39m get_source_code(metadata[\u001b[39m\"\u001b[39;49m\u001b[39mtag_name\u001b[39;49m\u001b[39m\"\u001b[39;49m], metadata[\u001b[39m\"\u001b[39;49m\u001b[39mfile_name\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m code_file_text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCode: \u001b[39m\u001b[39m{\u001b[39;00mcode_string\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb#X42sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m code_file_text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb 儲存格 30\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb#X42sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m spec\u001b[39m=\u001b[39mimportlib\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mspec_from_file_location(function_name, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(ctags_root_path, function_path))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb#X42sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m foo \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mmodule_from_spec(spec)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m spec\u001b[39m.\u001b[39;49mloader\u001b[39m.\u001b[39;49mexec_module(foo)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tony_kuo/pgithub/search-genius/lab1_azure_search.ipynb#X42sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mreturn\u001b[39;00m inspect\u001b[39m.\u001b[39mgetsource(foo)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/pgithub/search-genius/./repo/langchain/libs/langchain/langchain/text_splitter.py:642\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    635\u001b[0m                 Document(page_content\u001b[39m=\u001b[39mchunk[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m], metadata\u001b[39m=\u001b[39mchunk[\u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    636\u001b[0m                 \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m elements\n\u001b[1;32m    637\u001b[0m             ]\n\u001b[1;32m    640\u001b[0m \u001b[39m# should be in newer Python versions (3.10+)\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[39m# @dataclass(frozen=True, kw_only=True, slots=True)\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m \u001b[39m@dataclass\u001b[39;49m(frozen\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    643\u001b[0m \u001b[39mclass\u001b[39;49;00m \u001b[39mTokenizer\u001b[39;49;00m:\n\u001b[1;32m    644\u001b[0m \u001b[39m    \u001b[39;49m\u001b[39m\"\"\"Tokenizer data class.\"\"\"\u001b[39;49;00m\n\u001b[1;32m    646\u001b[0m     chunk_overlap: \u001b[39mint\u001b[39;49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/aicontest/lib/python3.11/dataclasses.py:1220\u001b[0m, in \u001b[0;36mdataclass.<locals>.wrap\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap\u001b[39m(\u001b[39mcls\u001b[39m):\n\u001b[0;32m-> 1220\u001b[0m     \u001b[39mreturn\u001b[39;00m _process_class(\u001b[39mcls\u001b[39;49m, init, \u001b[39mrepr\u001b[39;49m, eq, order, unsafe_hash,\n\u001b[1;32m   1221\u001b[0m                           frozen, match_args, kw_only, slots,\n\u001b[1;32m   1222\u001b[0m                           weakref_slot)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/aicontest/lib/python3.11/dataclasses.py:947\u001b[0m, in \u001b[0;36m_process_class\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m    942\u001b[0m dataclasses \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mmodules[\u001b[39m__name__\u001b[39m]\n\u001b[1;32m    943\u001b[0m \u001b[39mfor\u001b[39;00m name, \u001b[39mtype\u001b[39m \u001b[39min\u001b[39;00m cls_annotations\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    944\u001b[0m     \u001b[39m# See if this is a marker to change the value of kw_only.\u001b[39;00m\n\u001b[1;32m    945\u001b[0m     \u001b[39mif\u001b[39;00m (_is_kw_only(\u001b[39mtype\u001b[39m, dataclasses)\n\u001b[1;32m    946\u001b[0m         \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(\u001b[39mtype\u001b[39m, \u001b[39mstr\u001b[39m)\n\u001b[0;32m--> 947\u001b[0m             \u001b[39mand\u001b[39;00m _is_type(\u001b[39mtype\u001b[39;49m, \u001b[39mcls\u001b[39;49m, dataclasses, dataclasses\u001b[39m.\u001b[39;49mKW_ONLY,\n\u001b[1;32m    948\u001b[0m                          _is_kw_only))):\n\u001b[1;32m    949\u001b[0m         \u001b[39m# Switch the default to kw_only=True, and ignore this\u001b[39;00m\n\u001b[1;32m    950\u001b[0m         \u001b[39m# annotation: it's not a real field.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m         \u001b[39mif\u001b[39;00m KW_ONLY_seen:\n\u001b[1;32m    952\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m!r}\u001b[39;00m\u001b[39m is KW_ONLY, but KW_ONLY \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    953\u001b[0m                             \u001b[39m'\u001b[39m\u001b[39mhas already been specified\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/aicontest/lib/python3.11/dataclasses.py:712\u001b[0m, in \u001b[0;36m_is_type\u001b[0;34m(annotation, cls, a_module, a_type, is_type_predicate)\u001b[0m\n\u001b[1;32m    708\u001b[0m module_name \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m)\n\u001b[1;32m    709\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m module_name:\n\u001b[1;32m    710\u001b[0m     \u001b[39m# No module name, assume the class's module did\u001b[39;00m\n\u001b[1;32m    711\u001b[0m     \u001b[39m# \"from dataclasses import InitVar\".\u001b[39;00m\n\u001b[0;32m--> 712\u001b[0m     ns \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39;49mmodules\u001b[39m.\u001b[39;49mget(\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__module__\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__dict__\u001b[39;49m\n\u001b[1;32m    713\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    714\u001b[0m     \u001b[39m# Look up module_name in the class's module.\u001b[39;00m\n\u001b[1;32m    715\u001b[0m     module \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mmodules\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '__dict__'"
     ]
    }
   ],
   "source": [
    "user_question = \"What is Langchain design for?\"\n",
    "result = ask(user_question, retriever_type=\"hybrid_reranking_search\")\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(f\"👩‍💻 : {user_question}\")\n",
    "print(\"=\"*20)\n",
    "print(f\"🤖 : {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "👩‍💻 : How can i add a Custom Prompt Template in this repository? also add the unit-test. Give me an example\n",
      "====================\n",
      "🤖 : To add a custom prompt template in this repository, you can follow these steps:\n",
      "\n",
      "1. Create a new file in the relevant directory (e.g., langchain/chains/summarize/) and name it appropriately.\n",
      "2. In the new file, define a string variable that represents your prompt template. You can use triple quotes (\"\"\") to create multi-line strings for more complex templates.\n",
      "3. Use the `PromptTemplate.from_template()` method to create a prompt template object from your string template.\n",
      "4. Save the prompt template object to a variable for later use.\n",
      "\n",
      "Here's an example of adding a custom prompt template in the langchain/chains/summarize/refine_prompts.py file:\n",
      "\n",
      "```python\n",
      "from langchain_core.prompts import PromptTemplate\n",
      "\n",
      "custom_prompt_template = \"\"\"\\\n",
      "Your custom prompt template here\n",
      "\"\"\"\n",
      "\n",
      "CUSTOM_PROMPT = PromptTemplate.from_template(custom_prompt_template)\n",
      "```\n",
      "\n",
      "To add unit tests for your custom prompt template, you can create a separate test file in the tests directory and write test cases to validate the behavior of your prompt template. You can use testing frameworks like pytest to run the tests.\n",
      "\n",
      "I hope this helps! Let me know if you have any more questions.\n",
      "\n",
      "Citations:\n",
      "langchain/chains/summarize/refine_prompts.py\n",
      "langchain/chains/chat_vector_db/prompts.py\n",
      "langchain/evaluation/qa/generate_prompt.py\n"
     ]
    }
   ],
   "source": [
    "user_question = \"How can i add a Custom Prompt Template in this repository? also add the unit-test. Give me an example\"\n",
    "result = ask(user_question, retriever_type=\"hybrid_reranking_search\")\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(f\"👩‍💻 : {user_question}\")\n",
    "print(\"=\"*20)\n",
    "print(f\"🤖 : {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicontest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
