{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctags_path = './repo/langchain/libs/langchain/tags'\n",
    "assert os.path.isfile(ctags_path), \"Please run `zsh download_example_rpo.sh` first\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tags_file(file_path):\n",
    "    with open(file_path, 'r', errors='ignore') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    tags = []\n",
    "    for line in lines:\n",
    "        if line.startswith('!'):  # Skip metadata lines\n",
    "            continue\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) >= 4:\n",
    "            tag_name = parts[0]\n",
    "            file_name = parts[1]\n",
    "            pattern = parts[2]\n",
    "            tags.append(dict(tag_name=tag_name, file_name=file_name, pattern=pattern))\n",
    "\n",
    "    return tags\n",
    "\n",
    "# Use the function\n",
    "\n",
    "ctags_root_path = os.path.dirname(ctags_path)\n",
    "tags = read_tags_file(ctags_path)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for tag in tags:\n",
    "    documents.append(Document(page_content=f\"{tag['file_name']} | {tag['tag_name']} \", metadata=tag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/aicontest/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# You can use HuggingFaceEmbeddings as embedding model, this will runnuing faster in POC\n",
    "# The performance is similar to OpenAIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", model_kwargs={\"device\": \"cpu\"})\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", chunk_size = 1)\n",
    "\n",
    "# https://openai.com/blog/introducing-text-and-code-embeddings\n",
    "# embeddings = OpenAIEmbeddings(model=\"code-search-ada-code-001\", chunk_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(\"./db/faiss\"):\n",
    "    db = FAISS.load_local(folder_path=\"./db/faiss\", embeddings=embeddings, index_name=\"poc\")\n",
    "else:\n",
    "    db = FAISS.from_documents(documents, embeddings)\n",
    "    db.save_local(folder_path=\"./db/faiss\", index_name=\"poc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can i add a Custom Prompt Template in this repository?\"\n",
    "docs = db.similarity_search(query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='langchain/prompts/prompt.py | from_template ', metadata={'tag_name': 'from_template', 'file_name': 'langchain/prompts/prompt.py', 'pattern': '/^    def from_template(cls, template: str, **kwargs: Any) -> PromptTemplate:$/;\"'}),\n",
       " Document(page_content='langchain/prompts/prompt.py | PromptTemplate ', metadata={'tag_name': 'PromptTemplate', 'file_name': 'langchain/prompts/prompt.py', 'pattern': '/^class PromptTemplate(StringPromptTemplate):$/;\"'}),\n",
       " Document(page_content='langchain/schema/prompt_template.py | _prompt_type ', metadata={'tag_name': '_prompt_type', 'file_name': 'langchain/schema/prompt_template.py', 'pattern': '/^    def _prompt_type(self) -> str:$/;\"'}),\n",
       " Document(page_content='langchain/schema/prompt_template.py | BasePromptTemplate ', metadata={'tag_name': 'BasePromptTemplate', 'file_name': 'langchain/schema/prompt_template.py', 'pattern': '/^class BasePromptTemplate(Serializable, ABC):$/;\"'}),\n",
       " Document(page_content='langchain/prompts/prompt.py | template_format ', metadata={'tag_name': 'template_format', 'file_name': 'langchain/prompts/prompt.py', 'pattern': '/^    template_format: str = \"f-string\"$/;\"'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import inspect\n",
    "import importlib.util\n",
    "import os\n",
    "\n",
    "def get_source_code(function_name, function_path, ctags_root_path=ctags_root_path):\n",
    "    spec=importlib.util.spec_from_file_location(function_name, os.path.join(ctags_root_path, function_path))\n",
    "    foo = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(foo)\n",
    "    return inspect.getsource(foo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a hupful bot that fuilfill the human' program task:\n",
    "\n",
    "The following is releative code:\n",
    "{code_file_text}\n",
    "\n",
    "User: {user_prompt}\n",
    "Ai:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def create_code_file_text(docs : List[Document]):\n",
    "    code_file_text = \"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        code_file_text += f'==== File {i+1}/{len(docs)} ====\\n'\n",
    "        code_file_text += f'File path: {doc.metadata[\"file_name\"]}\\n'\n",
    "        code_file_text += f'Tag name: {doc.metadata[\"tag_name\"]}\\n'\n",
    "        code_string = get_source_code(doc.metadata[\"tag_name\"], doc.metadata[\"file_name\"])\n",
    "        code_file_text += f'Code: {code_string}\\n'\n",
    "        code_file_text += \"\\n\"\n",
    "    return code_file_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== File 1/1 ====\n",
      "File path: langchain/prompts/prompt.py\n",
      "Tag name: from_template\n",
      "Code: \"\"\"Prompt schema definition.\"\"\"\n",
      "from __future__ import annotations\n",
      "\n",
      "from pathlib import Path\n",
      "from string import Formatter\n",
      "from typing import Any, Dict, List, Union\n",
      "\n",
      "from pydantic import root_validator\n",
      "\n",
      "from langchain.prompts.base import (\n",
      "    DEFAULT_FORMATTER_MAPPING,\n",
      "    StringPromptTemplate,\n",
      "    _get_jinja2_variables_from_template,\n",
      "    check_valid_template,\n",
      ")\n",
      "\n",
      "\n",
      "class PromptTemplate(StringPromptTemplate):\n",
      "    \"\"\"Schema to represent a prompt for an LLM.\n",
      "\n",
      "    Example:\n",
      "        .. code-block:: python\n",
      "\n",
      "            from langchain import PromptTemplate\n",
      "            prompt = PromptTemplate(input_variables=[\"foo\"], template=\"Say {foo}\")\n",
      "    \"\"\"\n",
      "\n",
      "    @property\n",
      "    def lc_attributes(self) -> Dict[str, Any]:\n",
      "        return {\n",
      "            \"template_format\": self.template_format,\n",
      "        }\n",
      "\n",
      "    input_variables: List[str]\n",
      "    \"\"\"A list of the names of the variables the prompt template expects.\"\"\"\n",
      "\n",
      "    template: str\n",
      "    \"\"\"The prompt template.\"\"\"\n",
      "\n",
      "    template_format: str = \"f-string\"\n",
      "    \"\"\"The format of the prompt template. Options are: 'f-string', 'jinja2'.\"\"\"\n",
      "\n",
      "    validate_template: bool = True\n",
      "    \"\"\"Whether or not to try validating the template.\"\"\"\n",
      "\n",
      "    @property\n",
      "    def _prompt_type(self) -> str:\n",
      "        \"\"\"Return the prompt type key.\"\"\"\n",
      "        return \"prompt\"\n",
      "\n",
      "    def format(self, **kwargs: Any) -> str:\n",
      "        \"\"\"Format the prompt with the inputs.\n",
      "\n",
      "        Args:\n",
      "            kwargs: Any arguments to be passed to the prompt template.\n",
      "\n",
      "        Returns:\n",
      "            A formatted string.\n",
      "\n",
      "        Example:\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            prompt.format(variable1=\"foo\")\n",
      "        \"\"\"\n",
      "        kwargs = self._merge_partial_and_user_variables(**kwargs)\n",
      "        return DEFAULT_FORMATTER_MAPPING[self.template_format](self.template, **kwargs)\n",
      "\n",
      "    @root_validator()\n",
      "    def template_is_valid(cls, values: Dict) -> Dict:\n",
      "        \"\"\"Check that template and input variables are consistent.\"\"\"\n",
      "        if values[\"validate_template\"]:\n",
      "            all_inputs = values[\"input_variables\"] + list(values[\"partial_variables\"])\n",
      "            check_valid_template(\n",
      "                values[\"template\"], values[\"template_format\"], all_inputs\n",
      "            )\n",
      "        return values\n",
      "\n",
      "    @classmethod\n",
      "    def from_examples(\n",
      "        cls,\n",
      "        examples: List[str],\n",
      "        suffix: str,\n",
      "        input_variables: List[str],\n",
      "        example_separator: str = \"\\n\\n\",\n",
      "        prefix: str = \"\",\n",
      "        **kwargs: Any,\n",
      "    ) -> PromptTemplate:\n",
      "        \"\"\"Take examples in list format with prefix and suffix to create a prompt.\n",
      "\n",
      "        Intended to be used as a way to dynamically create a prompt from examples.\n",
      "\n",
      "        Args:\n",
      "            examples: List of examples to use in the prompt.\n",
      "            suffix: String to go after the list of examples. Should generally\n",
      "                set up the user's input.\n",
      "            input_variables: A list of variable names the final prompt template\n",
      "                will expect.\n",
      "            example_separator: The separator to use in between examples. Defaults\n",
      "                to two new line characters.\n",
      "            prefix: String that should go before any examples. Generally includes\n",
      "                examples. Default to an empty string.\n",
      "\n",
      "        Returns:\n",
      "            The final prompt generated.\n",
      "        \"\"\"\n",
      "        template = example_separator.join([prefix, *examples, suffix])\n",
      "        return cls(input_variables=input_variables, template=template, **kwargs)\n",
      "\n",
      "    @classmethod\n",
      "    def from_file(\n",
      "        cls, template_file: Union[str, Path], input_variables: List[str], **kwargs: Any\n",
      "    ) -> PromptTemplate:\n",
      "        \"\"\"Load a prompt from a file.\n",
      "\n",
      "        Args:\n",
      "            template_file: The path to the file containing the prompt template.\n",
      "            input_variables: A list of variable names the final prompt template\n",
      "                will expect.\n",
      "        Returns:\n",
      "            The prompt loaded from the file.\n",
      "        \"\"\"\n",
      "        with open(str(template_file), \"r\") as f:\n",
      "            template = f.read()\n",
      "        return cls(input_variables=input_variables, template=template, **kwargs)\n",
      "\n",
      "    @classmethod\n",
      "    def from_template(cls, template: str, **kwargs: Any) -> PromptTemplate:\n",
      "        \"\"\"Load a prompt template from a template.\"\"\"\n",
      "        if \"template_format\" in kwargs and kwargs[\"template_format\"] == \"jinja2\":\n",
      "            # Get the variables for the template\n",
      "            input_variables = _get_jinja2_variables_from_template(template)\n",
      "\n",
      "        else:\n",
      "            input_variables = {\n",
      "                v for _, v, _, _ in Formatter().parse(template) if v is not None\n",
      "            }\n",
      "\n",
      "        if \"partial_variables\" in kwargs:\n",
      "            partial_variables = kwargs[\"partial_variables\"]\n",
      "            input_variables = {\n",
      "                var for var in input_variables if var not in partial_variables\n",
      "            }\n",
      "\n",
      "        return cls(\n",
      "            input_variables=list(sorted(input_variables)), template=template, **kwargs\n",
      "        )\n",
      "\n",
      "\n",
      "# For backwards compatibility.\n",
      "Prompt = PromptTemplate\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code_text = create_code_file_text(docs=docs[:2])\n",
    "print(code_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/aicontest/lib/python3.11/site-packages/langchain/chat_models/azure_openai.py:155: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://f19855e6-c488-4c48-a0f0-e7bb2b9527fa-canadaeast.openai.azure.com to https://f19855e6-c488-4c48-a0f0-e7bb2b9527fa-canadaeast.openai.azure.com/openai.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/aicontest/lib/python3.11/site-packages/langchain/chat_models/azure_openai.py:162: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/aicontest/lib/python3.11/site-packages/langchain/chat_models/azure_openai.py:170: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://f19855e6-c488-4c48-a0f0-e7bb2b9527fa-canadaeast.openai.azure.com to https://f19855e6-c488-4c48-a0f0-e7bb2b9527fa-canadaeast.openai.azure.com/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=os.environ.get(\"DEPLOYMENT_NAME\"),\n",
    "    temperature=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(user_prompt: str) -> AIMessage:\n",
    "    # find docs similar to user_prompt\n",
    "    docs = db.similarity_search(query, k=4)\n",
    "    user_prompt = template.format(code_file_text=create_code_file_text(docs), user_prompt=user_prompt)\n",
    "\n",
    "    # call openai api here\n",
    "    message = HumanMessage(content=user_prompt)\n",
    "    return llm([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "üë©‚Äçüíª : What is Langchain design for?\n",
      "====================\n",
      "ü§ñ : Langchain appears to be a software library for language model prompts. It provides a structured way to define and manage prompts for language models. The code contains a class `PromptTemplate` which is a schema to represent a prompt for a Language Model (LLM). It allows users to define input variables, the template format, and whether to validate the template. \n",
      "\n",
      "The class also has methods to format the prompt with inputs, validate that the template and input variables are consistent, and create a prompt from examples, a file, or a template. The `BasePromptTemplate` class is an abstract base class that defines the basic structure and functionalities of a prompt template.\n",
      "\n",
      "In summary, Langchain is designed to provide a structured and reusable way to handle prompts for language models, making it easier to generate and validate prompts.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What is Langchain design for?\"\n",
    "result = ask(user_question).content\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(f\"üë©‚Äçüíª : {user_question}\")\n",
    "print(\"=\"*20)\n",
    "print(f\"ü§ñ : {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"How can i add a Custom Prompt Template in this repository? also add the unit-test. Give me an example\"\n",
    "result = ask(user_question).content\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(f\"üë©‚Äçüíª : {user_question}\")\n",
    "print(\"=\"*20)\n",
    "print(f\"ü§ñ : {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"What is the high-level system architecture of this project? Give me an example\"\n",
    "result = ask(user_question).content\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(f\"üë©‚Äçüíª : {user_question}\")\n",
    "print(\"=\"*20)\n",
    "print(f\"ü§ñ : {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of GPT-3.5 not knowing the langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I do not have any information about a term or concept called \"langchain.\" Could you please provide more context or details about what you are referring to?\n"
     ]
    }
   ],
   "source": [
    "print(llm([HumanMessage(content=\"What is langchain\")]).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
