{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctags_path = './repo/langchain/libs/langchain/tags'\n",
    "assert os.path.isfile(ctags_path), \"Please run `zsh download_example_rpo.sh` first\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tags_file(file_path):\n",
    "    with open(file_path, 'r', errors='ignore') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    tags = []\n",
    "    for line in lines:\n",
    "        if line.startswith('!'):  # Skip metadata lines\n",
    "            continue\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) >= 4:\n",
    "            tag_name = parts[0]\n",
    "            file_name = parts[1]\n",
    "            pattern = parts[2]\n",
    "            tags.append(dict(tag_name=tag_name, file_name=file_name, pattern=pattern))\n",
    "\n",
    "    return tags\n",
    "\n",
    "# Use the function\n",
    "\n",
    "ctags_root_path = os.path.dirname(ctags_path)\n",
    "tags = read_tags_file(ctags_path)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for tag in tags:\n",
    "    documents.append(Document(page_content=f\"{tag['file_name']} | {tag['tag_name']} \", metadata=tag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/aicontest/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# You can use HuggingFaceEmbeddings as embedding model, this will runnuing faster in POC\n",
    "# The performance is similar to OpenAIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", model_kwargs={\"device\": \"cpu\"})\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", chunk_size = 1)\n",
    "\n",
    "# https://openai.com/blog/introducing-text-and-code-embeddings\n",
    "# embeddings = OpenAIEmbeddings(model=\"code-search-ada-code-001\", chunk_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(\"./db/faiss\"):\n",
    "    db = FAISS.load_local(folder_path=\"./db/faiss\", embeddings=embeddings, index_name=\"poc\")\n",
    "else:\n",
    "    db = FAISS.from_documents(documents, embeddings)\n",
    "    db.save_local(folder_path=\"./db/faiss\", index_name=\"poc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can i add a Custom Prompt Template in this repository?\"\n",
    "docs = db.similarity_search(query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='langchain/prompts/prompt.py | from_template ', metadata={'tag_name': 'from_template', 'file_name': 'langchain/prompts/prompt.py', 'pattern': '/^    def from_template(cls, template: str, **kwargs: Any) -> PromptTemplate:$/;\"'}),\n",
       " Document(page_content='langchain/prompts/prompt.py | PromptTemplate ', metadata={'tag_name': 'PromptTemplate', 'file_name': 'langchain/prompts/prompt.py', 'pattern': '/^class PromptTemplate(StringPromptTemplate):$/;\"'}),\n",
       " Document(page_content='langchain/schema/prompt_template.py | _prompt_type ', metadata={'tag_name': '_prompt_type', 'file_name': 'langchain/schema/prompt_template.py', 'pattern': '/^    def _prompt_type(self) -> str:$/;\"'}),\n",
       " Document(page_content='langchain/schema/prompt_template.py | BasePromptTemplate ', metadata={'tag_name': 'BasePromptTemplate', 'file_name': 'langchain/schema/prompt_template.py', 'pattern': '/^class BasePromptTemplate(Serializable, ABC):$/;\"'}),\n",
       " Document(page_content='langchain/prompts/prompt.py | template_format ', metadata={'tag_name': 'template_format', 'file_name': 'langchain/prompts/prompt.py', 'pattern': '/^    template_format: str = \"f-string\"$/;\"'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import inspect\n",
    "import importlib.util\n",
    "import os\n",
    "\n",
    "def get_source_code(function_name, function_path, ctags_root_path=ctags_root_path):\n",
    "    spec=importlib.util.spec_from_file_location(function_name, os.path.join(ctags_root_path, function_path))\n",
    "    foo = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(foo)\n",
    "    return inspect.getsource(foo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a hupful bot that fuilfill the human' program task:\n",
    "\n",
    "The following is releative code:\n",
    "{code_file_text}\n",
    "\n",
    "User: {user_prompt}\n",
    "Ai:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def create_code_file_text(docs : List[Document]):\n",
    "    code_file_text = \"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        code_file_text += f'==== File {i+1}/{len(docs)} ====\\n'\n",
    "        code_file_text += f'File path: {doc.metadata[\"file_name\"]}\\n'\n",
    "        code_file_text += f'Tag name: {doc.metadata[\"tag_name\"]}\\n'\n",
    "        code_string = get_source_code(doc.metadata[\"tag_name\"], doc.metadata[\"file_name\"])\n",
    "        code_file_text += f'Code: {code_string}\\n'\n",
    "        code_file_text += \"\\n\"\n",
    "    return code_file_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== File 1/2 ====\n",
      "File path: langchain/prompts/prompt.py\n",
      "Tag name: from_template\n",
      "Code: \"\"\"Prompt schema definition.\"\"\"\n",
      "from __future__ import annotations\n",
      "\n",
      "from pathlib import Path\n",
      "from string import Formatter\n",
      "from typing import Any, Dict, List, Union\n",
      "\n",
      "from pydantic import root_validator\n",
      "\n",
      "from langchain.prompts.base import (\n",
      "    DEFAULT_FORMATTER_MAPPING,\n",
      "    StringPromptTemplate,\n",
      "    _get_jinja2_variables_from_template,\n",
      "    check_valid_template,\n",
      ")\n",
      "\n",
      "\n",
      "class PromptTemplate(StringPromptTemplate):\n",
      "    \"\"\"Schema to represent a prompt for an LLM.\n",
      "\n",
      "    Example:\n",
      "        .. code-block:: python\n",
      "\n",
      "            from langchain import PromptTemplate\n",
      "            prompt = PromptTemplate(input_variables=[\"foo\"], template=\"Say {foo}\")\n",
      "    \"\"\"\n",
      "\n",
      "    @property\n",
      "    def lc_attributes(self) -> Dict[str, Any]:\n",
      "        return {\n",
      "            \"template_format\": self.template_format,\n",
      "        }\n",
      "\n",
      "    input_variables: List[str]\n",
      "    \"\"\"A list of the names of the variables the prompt template expects.\"\"\"\n",
      "\n",
      "    template: str\n",
      "    \"\"\"The prompt template.\"\"\"\n",
      "\n",
      "    template_format: str = \"f-string\"\n",
      "    \"\"\"The format of the prompt template. Options are: 'f-string', 'jinja2'.\"\"\"\n",
      "\n",
      "    validate_template: bool = True\n",
      "    \"\"\"Whether or not to try validating the template.\"\"\"\n",
      "\n",
      "    @property\n",
      "    def _prompt_type(self) -> str:\n",
      "        \"\"\"Return the prompt type key.\"\"\"\n",
      "        return \"prompt\"\n",
      "\n",
      "    def format(self, **kwargs: Any) -> str:\n",
      "        \"\"\"Format the prompt with the inputs.\n",
      "\n",
      "        Args:\n",
      "            kwargs: Any arguments to be passed to the prompt template.\n",
      "\n",
      "        Returns:\n",
      "            A formatted string.\n",
      "\n",
      "        Example:\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            prompt.format(variable1=\"foo\")\n",
      "        \"\"\"\n",
      "        kwargs = self._merge_partial_and_user_variables(**kwargs)\n",
      "        return DEFAULT_FORMATTER_MAPPING[self.template_format](self.template, **kwargs)\n",
      "\n",
      "    @root_validator()\n",
      "    def template_is_valid(cls, values: Dict) -> Dict:\n",
      "        \"\"\"Check that template and input variables are consistent.\"\"\"\n",
      "        if values[\"validate_template\"]:\n",
      "            all_inputs = values[\"input_variables\"] + list(values[\"partial_variables\"])\n",
      "            check_valid_template(\n",
      "                values[\"template\"], values[\"template_format\"], all_inputs\n",
      "            )\n",
      "        return values\n",
      "\n",
      "    @classmethod\n",
      "    def from_examples(\n",
      "        cls,\n",
      "        examples: List[str],\n",
      "        suffix: str,\n",
      "        input_variables: List[str],\n",
      "        example_separator: str = \"\\n\\n\",\n",
      "        prefix: str = \"\",\n",
      "        **kwargs: Any,\n",
      "    ) -> PromptTemplate:\n",
      "        \"\"\"Take examples in list format with prefix and suffix to create a prompt.\n",
      "\n",
      "        Intended to be used as a way to dynamically create a prompt from examples.\n",
      "\n",
      "        Args:\n",
      "            examples: List of examples to use in the prompt.\n",
      "            suffix: String to go after the list of examples. Should generally\n",
      "                set up the user's input.\n",
      "            input_variables: A list of variable names the final prompt template\n",
      "                will expect.\n",
      "            example_separator: The separator to use in between examples. Defaults\n",
      "                to two new line characters.\n",
      "            prefix: String that should go before any examples. Generally includes\n",
      "                examples. Default to an empty string.\n",
      "\n",
      "        Returns:\n",
      "            The final prompt generated.\n",
      "        \"\"\"\n",
      "        template = example_separator.join([prefix, *examples, suffix])\n",
      "        return cls(input_variables=input_variables, template=template, **kwargs)\n",
      "\n",
      "    @classmethod\n",
      "    def from_file(\n",
      "        cls, template_file: Union[str, Path], input_variables: List[str], **kwargs: Any\n",
      "    ) -> PromptTemplate:\n",
      "        \"\"\"Load a prompt from a file.\n",
      "\n",
      "        Args:\n",
      "            template_file: The path to the file containing the prompt template.\n",
      "            input_variables: A list of variable names the final prompt template\n",
      "                will expect.\n",
      "        Returns:\n",
      "            The prompt loaded from the file.\n",
      "        \"\"\"\n",
      "        with open(str(template_file), \"r\") as f:\n",
      "            template = f.read()\n",
      "        return cls(input_variables=input_variables, template=template, **kwargs)\n",
      "\n",
      "    @classmethod\n",
      "    def from_template(cls, template: str, **kwargs: Any) -> PromptTemplate:\n",
      "        \"\"\"Load a prompt template from a template.\"\"\"\n",
      "        if \"template_format\" in kwargs and kwargs[\"template_format\"] == \"jinja2\":\n",
      "            # Get the variables for the template\n",
      "            input_variables = _get_jinja2_variables_from_template(template)\n",
      "\n",
      "        else:\n",
      "            input_variables = {\n",
      "                v for _, v, _, _ in Formatter().parse(template) if v is not None\n",
      "            }\n",
      "\n",
      "        if \"partial_variables\" in kwargs:\n",
      "            partial_variables = kwargs[\"partial_variables\"]\n",
      "            input_variables = {\n",
      "                var for var in input_variables if var not in partial_variables\n",
      "            }\n",
      "\n",
      "        return cls(\n",
      "            input_variables=list(sorted(input_variables)), template=template, **kwargs\n",
      "        )\n",
      "\n",
      "\n",
      "# For backwards compatibility.\n",
      "Prompt = PromptTemplate\n",
      "\n",
      "\n",
      "==== File 2/2 ====\n",
      "File path: langchain/prompts/prompt.py\n",
      "Tag name: PromptTemplate\n",
      "Code: \"\"\"Prompt schema definition.\"\"\"\n",
      "from __future__ import annotations\n",
      "\n",
      "from pathlib import Path\n",
      "from string import Formatter\n",
      "from typing import Any, Dict, List, Union\n",
      "\n",
      "from pydantic import root_validator\n",
      "\n",
      "from langchain.prompts.base import (\n",
      "    DEFAULT_FORMATTER_MAPPING,\n",
      "    StringPromptTemplate,\n",
      "    _get_jinja2_variables_from_template,\n",
      "    check_valid_template,\n",
      ")\n",
      "\n",
      "\n",
      "class PromptTemplate(StringPromptTemplate):\n",
      "    \"\"\"Schema to represent a prompt for an LLM.\n",
      "\n",
      "    Example:\n",
      "        .. code-block:: python\n",
      "\n",
      "            from langchain import PromptTemplate\n",
      "            prompt = PromptTemplate(input_variables=[\"foo\"], template=\"Say {foo}\")\n",
      "    \"\"\"\n",
      "\n",
      "    @property\n",
      "    def lc_attributes(self) -> Dict[str, Any]:\n",
      "        return {\n",
      "            \"template_format\": self.template_format,\n",
      "        }\n",
      "\n",
      "    input_variables: List[str]\n",
      "    \"\"\"A list of the names of the variables the prompt template expects.\"\"\"\n",
      "\n",
      "    template: str\n",
      "    \"\"\"The prompt template.\"\"\"\n",
      "\n",
      "    template_format: str = \"f-string\"\n",
      "    \"\"\"The format of the prompt template. Options are: 'f-string', 'jinja2'.\"\"\"\n",
      "\n",
      "    validate_template: bool = True\n",
      "    \"\"\"Whether or not to try validating the template.\"\"\"\n",
      "\n",
      "    @property\n",
      "    def _prompt_type(self) -> str:\n",
      "        \"\"\"Return the prompt type key.\"\"\"\n",
      "        return \"prompt\"\n",
      "\n",
      "    def format(self, **kwargs: Any) -> str:\n",
      "        \"\"\"Format the prompt with the inputs.\n",
      "\n",
      "        Args:\n",
      "            kwargs: Any arguments to be passed to the prompt template.\n",
      "\n",
      "        Returns:\n",
      "            A formatted string.\n",
      "\n",
      "        Example:\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            prompt.format(variable1=\"foo\")\n",
      "        \"\"\"\n",
      "        kwargs = self._merge_partial_and_user_variables(**kwargs)\n",
      "        return DEFAULT_FORMATTER_MAPPING[self.template_format](self.template, **kwargs)\n",
      "\n",
      "    @root_validator()\n",
      "    def template_is_valid(cls, values: Dict) -> Dict:\n",
      "        \"\"\"Check that template and input variables are consistent.\"\"\"\n",
      "        if values[\"validate_template\"]:\n",
      "            all_inputs = values[\"input_variables\"] + list(values[\"partial_variables\"])\n",
      "            check_valid_template(\n",
      "                values[\"template\"], values[\"template_format\"], all_inputs\n",
      "            )\n",
      "        return values\n",
      "\n",
      "    @classmethod\n",
      "    def from_examples(\n",
      "        cls,\n",
      "        examples: List[str],\n",
      "        suffix: str,\n",
      "        input_variables: List[str],\n",
      "        example_separator: str = \"\\n\\n\",\n",
      "        prefix: str = \"\",\n",
      "        **kwargs: Any,\n",
      "    ) -> PromptTemplate:\n",
      "        \"\"\"Take examples in list format with prefix and suffix to create a prompt.\n",
      "\n",
      "        Intended to be used as a way to dynamically create a prompt from examples.\n",
      "\n",
      "        Args:\n",
      "            examples: List of examples to use in the prompt.\n",
      "            suffix: String to go after the list of examples. Should generally\n",
      "                set up the user's input.\n",
      "            input_variables: A list of variable names the final prompt template\n",
      "                will expect.\n",
      "            example_separator: The separator to use in between examples. Defaults\n",
      "                to two new line characters.\n",
      "            prefix: String that should go before any examples. Generally includes\n",
      "                examples. Default to an empty string.\n",
      "\n",
      "        Returns:\n",
      "            The final prompt generated.\n",
      "        \"\"\"\n",
      "        template = example_separator.join([prefix, *examples, suffix])\n",
      "        return cls(input_variables=input_variables, template=template, **kwargs)\n",
      "\n",
      "    @classmethod\n",
      "    def from_file(\n",
      "        cls, template_file: Union[str, Path], input_variables: List[str], **kwargs: Any\n",
      "    ) -> PromptTemplate:\n",
      "        \"\"\"Load a prompt from a file.\n",
      "\n",
      "        Args:\n",
      "            template_file: The path to the file containing the prompt template.\n",
      "            input_variables: A list of variable names the final prompt template\n",
      "                will expect.\n",
      "        Returns:\n",
      "            The prompt loaded from the file.\n",
      "        \"\"\"\n",
      "        with open(str(template_file), \"r\") as f:\n",
      "            template = f.read()\n",
      "        return cls(input_variables=input_variables, template=template, **kwargs)\n",
      "\n",
      "    @classmethod\n",
      "    def from_template(cls, template: str, **kwargs: Any) -> PromptTemplate:\n",
      "        \"\"\"Load a prompt template from a template.\"\"\"\n",
      "        if \"template_format\" in kwargs and kwargs[\"template_format\"] == \"jinja2\":\n",
      "            # Get the variables for the template\n",
      "            input_variables = _get_jinja2_variables_from_template(template)\n",
      "\n",
      "        else:\n",
      "            input_variables = {\n",
      "                v for _, v, _, _ in Formatter().parse(template) if v is not None\n",
      "            }\n",
      "\n",
      "        if \"partial_variables\" in kwargs:\n",
      "            partial_variables = kwargs[\"partial_variables\"]\n",
      "            input_variables = {\n",
      "                var for var in input_variables if var not in partial_variables\n",
      "            }\n",
      "\n",
      "        return cls(\n",
      "            input_variables=list(sorted(input_variables)), template=template, **kwargs\n",
      "        )\n",
      "\n",
      "\n",
      "# For backwards compatibility.\n",
      "Prompt = PromptTemplate\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code_text = create_code_file_text(docs=docs[:2])\n",
    "print(code_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/aicontest/lib/python3.11/site-packages/langchain/chat_models/azure_openai.py:155: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://f19855e6-c488-4c48-a0f0-e7bb2b9527fa-canadaeast.openai.azure.com to https://f19855e6-c488-4c48-a0f0-e7bb2b9527fa-canadaeast.openai.azure.com/openai.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/aicontest/lib/python3.11/site-packages/langchain/chat_models/azure_openai.py:162: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/aicontest/lib/python3.11/site-packages/langchain/chat_models/azure_openai.py:170: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://f19855e6-c488-4c48-a0f0-e7bb2b9527fa-canadaeast.openai.azure.com to https://f19855e6-c488-4c48-a0f0-e7bb2b9527fa-canadaeast.openai.azure.com/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=os.environ.get(\"DEPLOYMENT_NAME\"),\n",
    "    temperature=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(user_prompt: str) -> AIMessage:\n",
    "    # find docs similar to user_prompt\n",
    "    docs = db.similarity_search(query, k=4)\n",
    "    user_prompt = template.format(code_file_text=create_code_file_text(docs), user_prompt=user_prompt)\n",
    "\n",
    "    # call openai api here\n",
    "    message = HumanMessage(content=user_prompt)\n",
    "    return llm([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "👩‍💻 : What is Langchain design for?\n",
      "====================\n",
      "🤖 : Langchain is designed to be a language model development framework. It provides tools and libraries for creating and training language models, as well as generating prompts and processing outputs. It aims to simplify the process of building and deploying language models for various natural language processing tasks.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What is Langchain design for?\"\n",
    "result = ask(user_question).content\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(f\"👩‍💻 : {user_question}\")\n",
    "print(\"=\"*20)\n",
    "print(f\"🤖 : {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "👩‍💻 : How can i add a Custom Prompt Template in this repository? also add the unit-test. Give me an example\n",
      "====================\n",
      "🤖 : To add a custom prompt template to this repository, you can follow these steps:\n",
      "\n",
      "1. Create a new Python file in the appropriate directory, such as `langchain/prompts`.\n",
      "2. Define your custom prompt template class by inheriting from `BasePromptTemplate`.\n",
      "3. Implement the required methods and properties of the `BasePromptTemplate` class, such as `format_prompt` and `_prompt_type`.\n",
      "4. Add any additional methods or properties specific to your custom prompt template.\n",
      "5. Write unit tests for your custom prompt template to ensure its functionality.\n",
      "6. Save the file and commit it to the repository.\n",
      "\n",
      "Here's an example of a custom prompt template class and its corresponding unit test:\n",
      "\n",
      "```python\n",
      "# File: langchain/prompts/custom_prompt.py\n",
      "\n",
      "from langchain.schema.prompt_template import BasePromptTemplate\n",
      "\n",
      "class CustomPromptTemplate(BasePromptTemplate):\n",
      "    def format_prompt(self, **kwargs):\n",
      "        # Implement the logic to format the prompt based on the input variables\n",
      "        # and return the formatted prompt as a string or a PromptValue object.\n",
      "        pass\n",
      "\n",
      "    @property\n",
      "    def _prompt_type(self):\n",
      "        return \"custom_prompt\"\n",
      "\n",
      "\n",
      "# File: tests/test_custom_prompt.py\n",
      "\n",
      "import pytest\n",
      "from langchain.prompts.custom_prompt import CustomPromptTemplate\n",
      "\n",
      "def test_custom_prompt_format_prompt():\n",
      "    prompt = CustomPromptTemplate(input_variables=[\"variable1\", \"variable2\"], template=\"This is a {variable1} with {variable2}\")\n",
      "    formatted_prompt = prompt.format_prompt(variable1=\"custom\", variable2=\"template\")\n",
      "    assert formatted_prompt == \"This is a custom with template\"\n",
      "\n",
      "```\n",
      "\n",
      "In this example, we created a custom prompt template class called `CustomPromptTemplate` that inherits from `BasePromptTemplate`. We implemented the `format_prompt` method to format the prompt based on the input variables and return the formatted prompt. We also defined the `_prompt_type` property to specify the type of the prompt template.\n",
      "\n",
      "The unit test `test_custom_prompt_format_prompt` tests the `format_prompt` method by creating an instance of the `CustomPromptTemplate` class and calling the `format_prompt` method with sample input variables. It asserts that the formatted prompt matches the expected output.\n",
      "\n",
      "Remember to update the import statements and file paths according to your project structure.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"How can i add a Custom Prompt Template in this repository? also add the unit-test. Give me an example\"\n",
    "result = ask(user_question).content\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(f\"👩‍💻 : {user_question}\")\n",
    "print(\"=\"*20)\n",
    "print(f\"🤖 : {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "👩‍💻 : What is the high-level system architecture of this project? Give me an example\n",
      "====================\n",
      "🤖 : The high-level system architecture of this project consists of several components. Here is an example:\n",
      "\n",
      "1. File 1/4: `langchain/prompts/prompt.py`\n",
      "   - Contains the definition of the `PromptTemplate` class, which represents a prompt for an LLM (Language Model).\n",
      "   - It includes properties such as `input_variables`, `template`, `template_format`, and `validate_template`.\n",
      "   - It also has methods for formatting the prompt and validating the template.\n",
      "\n",
      "2. File 2/4: `langchain/prompts/prompt.py`\n",
      "   - Contains the same definition of the `PromptTemplate` class as in File 1/4.\n",
      "   - This duplication might be an error or a versioning issue.\n",
      "\n",
      "3. File 3/4: `langchain/schema/prompt_template.py`\n",
      "   - Defines the `BasePromptTemplate` class, which is the base class for all prompt templates.\n",
      "   - It includes properties such as `input_variables`, `output_parser`, and `partial_variables`.\n",
      "   - It also has methods for formatting the prompt and validating variable names.\n",
      "\n",
      "4. File 4/4: `langchain/schema/prompt_template.py`\n",
      "   - Contains the same definition of the `BasePromptTemplate` class as in File 3/4.\n",
      "   - This duplication might be an error or a versioning issue.\n",
      "\n",
      "These files represent the core components of the prompt template system in the project. The `PromptTemplate` and `BasePromptTemplate` classes define the structure and behavior of prompt templates, allowing users to create and format prompts for the LLM.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What is the high-level system architecture of this project? Give me an example\"\n",
    "result = ask(user_question).content\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(f\"👩‍💻 : {user_question}\")\n",
    "print(\"=\"*20)\n",
    "print(f\"🤖 : {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of GPT-3.5 not knowing the langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain is a decentralized blockchain platform that aims to provide a solution for language-related challenges in various industries. It aims to bridge the language gap by offering language services such as translation, interpretation, and proofreading through its network of language professionals. The platform utilizes smart contracts and blockchain technology to ensure secure and transparent transactions between clients and language service providers. Langchain also aims to create a global community of language professionals and facilitate the exchange of knowledge and expertise in different languages.\n"
     ]
    }
   ],
   "source": [
    "print(llm([HumanMessage(content=\"What is langchain\")]).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
