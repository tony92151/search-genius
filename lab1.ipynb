{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "\n",
    "# Uncomment the following line if you need to initialize FAISS with no AVX2 optimization\n",
    "# os.environ['FAISS_NO_AVX2'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "def read_tags_file(file_path):\n",
    "    with open(file_path, 'r', errors='ignore') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    tags = []\n",
    "    for line in lines:\n",
    "        if line.startswith('!'):  # Skip metadata lines\n",
    "            continue\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) >= 4:\n",
    "            tag_name = parts[0]\n",
    "            file_name = parts[1]\n",
    "            pattern = parts[2]\n",
    "            tags.append(dict(tag_name=tag_name, file_name=file_name, pattern=pattern))\n",
    "\n",
    "    return tags\n",
    "\n",
    "# Use the function\n",
    "tags = read_tags_file('./repo/langchain/libs/langchain/tags')\n",
    "\n",
    "documents = []\n",
    "\n",
    "for tag in tags:\n",
    "    documents.append(Document(page_content=f\"{tag['file_name']} | {tag['tag_name']} \", metadata=tag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can i add a Custom Prompt Template in this repository?\"\n",
    "docs = db.similarity_search(query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='langchain/prompts/prompt.py | from_template ', metadata={'tag_name': 'from_template', 'file_name': 'langchain/prompts/prompt.py', 'pattern': '/^    def from_template(cls, template: str, **kwargs: Any) -> PromptTemplate:$/;\"'}),\n",
       " Document(page_content='langchain/prompts/prompt.py | PromptTemplate ', metadata={'tag_name': 'PromptTemplate', 'file_name': 'langchain/prompts/prompt.py', 'pattern': '/^class PromptTemplate(StringPromptTemplate):$/;\"'}),\n",
       " Document(page_content='langchain/schema/prompt_template.py | _prompt_type ', metadata={'tag_name': '_prompt_type', 'file_name': 'langchain/schema/prompt_template.py', 'pattern': '/^    def _prompt_type(self) -> str:$/;\"'}),\n",
       " Document(page_content='langchain/schema/prompt_template.py | BasePromptTemplate ', metadata={'tag_name': 'BasePromptTemplate', 'file_name': 'langchain/schema/prompt_template.py', 'pattern': '/^class BasePromptTemplate(Serializable, ABC):$/;\"'}),\n",
       " Document(page_content='langchain/schema/prompt_template.py | format_prompt ', metadata={'tag_name': 'format_prompt', 'file_name': 'langchain/schema/prompt_template.py', 'pattern': '/^    def format_prompt(self, **kwargs: Any) -> PromptValue:$/;\"'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import inspect\n",
    "import importlib.util\n",
    "import os\n",
    "\n",
    "root = \"repo/langchain/libs/langchain\"\n",
    " \n",
    "\n",
    "def get_source_code(function_name, function_path):\n",
    "    spec=importlib.util.spec_from_file_location(function_name, os.path.join(root, function_path))\n",
    "    foo = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(foo)\n",
    "    return inspect.getsource(foo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a hupful bot that fuilfill the human' program task:\n",
    "\n",
    "The following is releative code:\n",
    "{code_file_text}\n",
    "\n",
    "User: {user_prompt}\n",
    "Ai:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def create_code_file_text(docs : List[Document]):\n",
    "    code_file_text = \"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        code_file_text += f'==== File {i+1}/{len(docs)} ====\\n'\n",
    "        code_file_text += f'File path: {doc.metadata[\"file_name\"]}\\n'\n",
    "        code_file_text += f'Tag name: {doc.metadata[\"tag_name\"]}\\n'\n",
    "        code_file_text += f'Code: {get_source_code(doc.metadata[\"tag_name\"], doc.metadata[\"file_name\"])}\\n'\n",
    "        code_file_text += \"\\n\"\n",
    "    return code_file_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_text = create_code_file_text(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "\n",
    "# llm = AzureChatOpenAI(\n",
    "#     openai_api_key=getpass.getpass(\"OpenAI API Key:\"),\n",
    "#     deployment_name=\"gpt35-chat\",\n",
    "#     openai_api_type=\"azure\",\n",
    "#     openai_api_base=getpass.getpass(\"OpenAI API Base:\"),\n",
    "#     openai_api_version=\"2023-03-15-preview\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(user_prompt):\n",
    "    # find docs similar to user_prompt\n",
    "    docs = db.similarity_search(query, k=5)\n",
    "    result = template.format(code_file_text=code_text, user_prompt=user_prompt)\n",
    "    # call openai api here\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a hupful bot that fuilfill the human\\' program task:\\n\\nThe following is releative code:\\n==== File 1/5 ====\\nFile path: langchain/prompts/prompt.py\\nTag name: from_template\\nCode: \"\"\"Prompt schema definition.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom pathlib import Path\\nfrom string import Formatter\\nfrom typing import Any, Dict, List, Union\\n\\nfrom pydantic import root_validator\\n\\nfrom langchain.prompts.base import (\\n    DEFAULT_FORMATTER_MAPPING,\\n    StringPromptTemplate,\\n    _get_jinja2_variables_from_template,\\n    check_valid_template,\\n)\\n\\n\\nclass PromptTemplate(StringPromptTemplate):\\n    \"\"\"Schema to represent a prompt for an LLM.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain import PromptTemplate\\n            prompt = PromptTemplate(input_variables=[\"foo\"], template=\"Say {foo}\")\\n    \"\"\"\\n\\n    @property\\n    def lc_attributes(self) -> Dict[str, Any]:\\n        return {\\n            \"template_format\": self.template_format,\\n        }\\n\\n    input_variables: List[str]\\n    \"\"\"A list of the names of the variables the prompt template expects.\"\"\"\\n\\n    template: str\\n    \"\"\"The prompt template.\"\"\"\\n\\n    template_format: str = \"f-string\"\\n    \"\"\"The format of the prompt template. Options are: \\'f-string\\', \\'jinja2\\'.\"\"\"\\n\\n    validate_template: bool = True\\n    \"\"\"Whether or not to try validating the template.\"\"\"\\n\\n    @property\\n    def _prompt_type(self) -> str:\\n        \"\"\"Return the prompt type key.\"\"\"\\n        return \"prompt\"\\n\\n    def format(self, **kwargs: Any) -> str:\\n        \"\"\"Format the prompt with the inputs.\\n\\n        Args:\\n            kwargs: Any arguments to be passed to the prompt template.\\n\\n        Returns:\\n            A formatted string.\\n\\n        Example:\\n\\n        .. code-block:: python\\n\\n            prompt.format(variable1=\"foo\")\\n        \"\"\"\\n        kwargs = self._merge_partial_and_user_variables(**kwargs)\\n        return DEFAULT_FORMATTER_MAPPING[self.template_format](self.template, **kwargs)\\n\\n    @root_validator()\\n    def template_is_valid(cls, values: Dict) -> Dict:\\n        \"\"\"Check that template and input variables are consistent.\"\"\"\\n        if values[\"validate_template\"]:\\n            all_inputs = values[\"input_variables\"] + list(values[\"partial_variables\"])\\n            check_valid_template(\\n                values[\"template\"], values[\"template_format\"], all_inputs\\n            )\\n        return values\\n\\n    @classmethod\\n    def from_examples(\\n        cls,\\n        examples: List[str],\\n        suffix: str,\\n        input_variables: List[str],\\n        example_separator: str = \"\\\\n\\\\n\",\\n        prefix: str = \"\",\\n        **kwargs: Any,\\n    ) -> PromptTemplate:\\n        \"\"\"Take examples in list format with prefix and suffix to create a prompt.\\n\\n        Intended to be used as a way to dynamically create a prompt from examples.\\n\\n        Args:\\n            examples: List of examples to use in the prompt.\\n            suffix: String to go after the list of examples. Should generally\\n                set up the user\\'s input.\\n            input_variables: A list of variable names the final prompt template\\n                will expect.\\n            example_separator: The separator to use in between examples. Defaults\\n                to two new line characters.\\n            prefix: String that should go before any examples. Generally includes\\n                examples. Default to an empty string.\\n\\n        Returns:\\n            The final prompt generated.\\n        \"\"\"\\n        template = example_separator.join([prefix, *examples, suffix])\\n        return cls(input_variables=input_variables, template=template, **kwargs)\\n\\n    @classmethod\\n    def from_file(\\n        cls, template_file: Union[str, Path], input_variables: List[str], **kwargs: Any\\n    ) -> PromptTemplate:\\n        \"\"\"Load a prompt from a file.\\n\\n        Args:\\n            template_file: The path to the file containing the prompt template.\\n            input_variables: A list of variable names the final prompt template\\n                will expect.\\n        Returns:\\n            The prompt loaded from the file.\\n        \"\"\"\\n        with open(str(template_file), \"r\") as f:\\n            template = f.read()\\n        return cls(input_variables=input_variables, template=template, **kwargs)\\n\\n    @classmethod\\n    def from_template(cls, template: str, **kwargs: Any) -> PromptTemplate:\\n        \"\"\"Load a prompt template from a template.\"\"\"\\n        if \"template_format\" in kwargs and kwargs[\"template_format\"] == \"jinja2\":\\n            # Get the variables for the template\\n            input_variables = _get_jinja2_variables_from_template(template)\\n\\n        else:\\n            input_variables = {\\n                v for _, v, _, _ in Formatter().parse(template) if v is not None\\n            }\\n\\n        if \"partial_variables\" in kwargs:\\n            partial_variables = kwargs[\"partial_variables\"]\\n            input_variables = {\\n                var for var in input_variables if var not in partial_variables\\n            }\\n\\n        return cls(\\n            input_variables=list(sorted(input_variables)), template=template, **kwargs\\n        )\\n\\n\\n# For backwards compatibility.\\nPrompt = PromptTemplate\\n\\n\\n==== File 2/5 ====\\nFile path: langchain/prompts/prompt.py\\nTag name: PromptTemplate\\nCode: \"\"\"Prompt schema definition.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom pathlib import Path\\nfrom string import Formatter\\nfrom typing import Any, Dict, List, Union\\n\\nfrom pydantic import root_validator\\n\\nfrom langchain.prompts.base import (\\n    DEFAULT_FORMATTER_MAPPING,\\n    StringPromptTemplate,\\n    _get_jinja2_variables_from_template,\\n    check_valid_template,\\n)\\n\\n\\nclass PromptTemplate(StringPromptTemplate):\\n    \"\"\"Schema to represent a prompt for an LLM.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain import PromptTemplate\\n            prompt = PromptTemplate(input_variables=[\"foo\"], template=\"Say {foo}\")\\n    \"\"\"\\n\\n    @property\\n    def lc_attributes(self) -> Dict[str, Any]:\\n        return {\\n            \"template_format\": self.template_format,\\n        }\\n\\n    input_variables: List[str]\\n    \"\"\"A list of the names of the variables the prompt template expects.\"\"\"\\n\\n    template: str\\n    \"\"\"The prompt template.\"\"\"\\n\\n    template_format: str = \"f-string\"\\n    \"\"\"The format of the prompt template. Options are: \\'f-string\\', \\'jinja2\\'.\"\"\"\\n\\n    validate_template: bool = True\\n    \"\"\"Whether or not to try validating the template.\"\"\"\\n\\n    @property\\n    def _prompt_type(self) -> str:\\n        \"\"\"Return the prompt type key.\"\"\"\\n        return \"prompt\"\\n\\n    def format(self, **kwargs: Any) -> str:\\n        \"\"\"Format the prompt with the inputs.\\n\\n        Args:\\n            kwargs: Any arguments to be passed to the prompt template.\\n\\n        Returns:\\n            A formatted string.\\n\\n        Example:\\n\\n        .. code-block:: python\\n\\n            prompt.format(variable1=\"foo\")\\n        \"\"\"\\n        kwargs = self._merge_partial_and_user_variables(**kwargs)\\n        return DEFAULT_FORMATTER_MAPPING[self.template_format](self.template, **kwargs)\\n\\n    @root_validator()\\n    def template_is_valid(cls, values: Dict) -> Dict:\\n        \"\"\"Check that template and input variables are consistent.\"\"\"\\n        if values[\"validate_template\"]:\\n            all_inputs = values[\"input_variables\"] + list(values[\"partial_variables\"])\\n            check_valid_template(\\n                values[\"template\"], values[\"template_format\"], all_inputs\\n            )\\n        return values\\n\\n    @classmethod\\n    def from_examples(\\n        cls,\\n        examples: List[str],\\n        suffix: str,\\n        input_variables: List[str],\\n        example_separator: str = \"\\\\n\\\\n\",\\n        prefix: str = \"\",\\n        **kwargs: Any,\\n    ) -> PromptTemplate:\\n        \"\"\"Take examples in list format with prefix and suffix to create a prompt.\\n\\n        Intended to be used as a way to dynamically create a prompt from examples.\\n\\n        Args:\\n            examples: List of examples to use in the prompt.\\n            suffix: String to go after the list of examples. Should generally\\n                set up the user\\'s input.\\n            input_variables: A list of variable names the final prompt template\\n                will expect.\\n            example_separator: The separator to use in between examples. Defaults\\n                to two new line characters.\\n            prefix: String that should go before any examples. Generally includes\\n                examples. Default to an empty string.\\n\\n        Returns:\\n            The final prompt generated.\\n        \"\"\"\\n        template = example_separator.join([prefix, *examples, suffix])\\n        return cls(input_variables=input_variables, template=template, **kwargs)\\n\\n    @classmethod\\n    def from_file(\\n        cls, template_file: Union[str, Path], input_variables: List[str], **kwargs: Any\\n    ) -> PromptTemplate:\\n        \"\"\"Load a prompt from a file.\\n\\n        Args:\\n            template_file: The path to the file containing the prompt template.\\n            input_variables: A list of variable names the final prompt template\\n                will expect.\\n        Returns:\\n            The prompt loaded from the file.\\n        \"\"\"\\n        with open(str(template_file), \"r\") as f:\\n            template = f.read()\\n        return cls(input_variables=input_variables, template=template, **kwargs)\\n\\n    @classmethod\\n    def from_template(cls, template: str, **kwargs: Any) -> PromptTemplate:\\n        \"\"\"Load a prompt template from a template.\"\"\"\\n        if \"template_format\" in kwargs and kwargs[\"template_format\"] == \"jinja2\":\\n            # Get the variables for the template\\n            input_variables = _get_jinja2_variables_from_template(template)\\n\\n        else:\\n            input_variables = {\\n                v for _, v, _, _ in Formatter().parse(template) if v is not None\\n            }\\n\\n        if \"partial_variables\" in kwargs:\\n            partial_variables = kwargs[\"partial_variables\"]\\n            input_variables = {\\n                var for var in input_variables if var not in partial_variables\\n            }\\n\\n        return cls(\\n            input_variables=list(sorted(input_variables)), template=template, **kwargs\\n        )\\n\\n\\n# For backwards compatibility.\\nPrompt = PromptTemplate\\n\\n\\n==== File 3/5 ====\\nFile path: langchain/schema/prompt_template.py\\nTag name: _prompt_type\\nCode: from __future__ import annotations\\n\\nimport json\\nfrom abc import ABC, abstractmethod\\nfrom pathlib import Path\\nfrom typing import Any, Callable, Dict, List, Mapping, Optional, Union\\n\\nimport yaml\\nfrom pydantic import Field, root_validator\\n\\nfrom langchain.load.serializable import Serializable\\nfrom langchain.schema.document import Document\\nfrom langchain.schema.output_parser import BaseOutputParser\\nfrom langchain.schema.prompt import PromptValue\\n\\n\\nclass BasePromptTemplate(Serializable, ABC):\\n    \"\"\"Base class for all prompt templates, returning a prompt.\"\"\"\\n\\n    input_variables: List[str]\\n    \"\"\"A list of the names of the variables the prompt template expects.\"\"\"\\n    output_parser: Optional[BaseOutputParser] = None\\n    \"\"\"How to parse the output of calling an LLM on this formatted prompt.\"\"\"\\n    partial_variables: Mapping[str, Union[str, Callable[[], str]]] = Field(\\n        default_factory=dict\\n    )\\n\\n    @property\\n    def lc_serializable(self) -> bool:\\n        return True\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True\\n\\n    @abstractmethod\\n    def format_prompt(self, **kwargs: Any) -> PromptValue:\\n        \"\"\"Create Chat Messages.\"\"\"\\n\\n    @root_validator()\\n    def validate_variable_names(cls, values: Dict) -> Dict:\\n        \"\"\"Validate variable names do not include restricted names.\"\"\"\\n        if \"stop\" in values[\"input_variables\"]:\\n            raise ValueError(\\n                \"Cannot have an input variable named \\'stop\\', as it is used internally,\"\\n                \" please rename.\"\\n            )\\n        if \"stop\" in values[\"partial_variables\"]:\\n            raise ValueError(\\n                \"Cannot have an partial variable named \\'stop\\', as it is used \"\\n                \"internally, please rename.\"\\n            )\\n\\n        overall = set(values[\"input_variables\"]).intersection(\\n            values[\"partial_variables\"]\\n        )\\n        if overall:\\n            raise ValueError(\\n                f\"Found overlapping input and partial variables: {overall}\"\\n            )\\n        return values\\n\\n    def partial(self, **kwargs: Union[str, Callable[[], str]]) -> BasePromptTemplate:\\n        \"\"\"Return a partial of the prompt template.\"\"\"\\n        prompt_dict = self.__dict__.copy()\\n        prompt_dict[\"input_variables\"] = list(\\n            set(self.input_variables).difference(kwargs)\\n        )\\n        prompt_dict[\"partial_variables\"] = {**self.partial_variables, **kwargs}\\n        return type(self)(**prompt_dict)\\n\\n    def _merge_partial_and_user_variables(self, **kwargs: Any) -> Dict[str, Any]:\\n        # Get partial params:\\n        partial_kwargs = {\\n            k: v if isinstance(v, str) else v()\\n            for k, v in self.partial_variables.items()\\n        }\\n        return {**partial_kwargs, **kwargs}\\n\\n    @abstractmethod\\n    def format(self, **kwargs: Any) -> str:\\n        \"\"\"Format the prompt with the inputs.\\n\\n        Args:\\n            kwargs: Any arguments to be passed to the prompt template.\\n\\n        Returns:\\n            A formatted string.\\n\\n        Example:\\n\\n        .. code-block:: python\\n\\n            prompt.format(variable1=\"foo\")\\n        \"\"\"\\n\\n    @property\\n    def _prompt_type(self) -> str:\\n        \"\"\"Return the prompt type key.\"\"\"\\n        raise NotImplementedError\\n\\n    def dict(self, **kwargs: Any) -> Dict:\\n        \"\"\"Return dictionary representation of prompt.\"\"\"\\n        prompt_dict = super().dict(**kwargs)\\n        prompt_dict[\"_type\"] = self._prompt_type\\n        return prompt_dict\\n\\n    def save(self, file_path: Union[Path, str]) -> None:\\n        \"\"\"Save the prompt.\\n\\n        Args:\\n            file_path: Path to directory to save prompt to.\\n\\n        Example:\\n        .. code-block:: python\\n\\n            prompt.save(file_path=\"path/prompt.yaml\")\\n        \"\"\"\\n        if self.partial_variables:\\n            raise ValueError(\"Cannot save prompt with partial variables.\")\\n        # Convert file to Path object.\\n        if isinstance(file_path, str):\\n            save_path = Path(file_path)\\n        else:\\n            save_path = file_path\\n\\n        directory_path = save_path.parent\\n        directory_path.mkdir(parents=True, exist_ok=True)\\n\\n        # Fetch dictionary to save\\n        prompt_dict = self.dict()\\n\\n        if save_path.suffix == \".json\":\\n            with open(file_path, \"w\") as f:\\n                json.dump(prompt_dict, f, indent=4)\\n        elif save_path.suffix == \".yaml\":\\n            with open(file_path, \"w\") as f:\\n                yaml.dump(prompt_dict, f, default_flow_style=False)\\n        else:\\n            raise ValueError(f\"{save_path} must be json or yaml\")\\n\\n\\ndef format_document(doc: Document, prompt: BasePromptTemplate) -> str:\\n    \"\"\"Format a document into a string based on a prompt template.\\n\\n    First, this pulls information from the document from two sources:\\n\\n    1. `page_content`:\\n        This takes the information from the `document.page_content`\\n        and assigns it to a variable named `page_content`.\\n    2. metadata:\\n        This takes information from `document.metadata` and assigns\\n        it to variables of the same name.\\n\\n    Those variables are then passed into the `prompt` to produce a formatted string.\\n\\n    Args:\\n        doc: Document, the page_content and metadata will be used to create\\n            the final string.\\n        prompt: BasePromptTemplate, will be used to format the page_content\\n            and metadata into the final string.\\n\\n    Returns:\\n        string of the document formatted.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.schema import Document\\n            from langchain.prompts import PromptTemplate\\n            doc = Document(page_content=\"This is a joke\", metadata={\"page\": \"1\"})\\n            prompt = PromptTemplate.from_template(\"Page {page}: {page_content}\")\\n            format_document(doc, prompt)\\n            >>> \"Page 1: This is a joke\"\\n    \"\"\"\\n    base_info = {\"page_content\": doc.page_content, **doc.metadata}\\n    missing_metadata = set(prompt.input_variables).difference(base_info)\\n    if len(missing_metadata) > 0:\\n        required_metadata = [\\n            iv for iv in prompt.input_variables if iv != \"page_content\"\\n        ]\\n        raise ValueError(\\n            f\"Document prompt requires documents to have metadata variables: \"\\n            f\"{required_metadata}. Received document with missing metadata: \"\\n            f\"{list(missing_metadata)}.\"\\n        )\\n    document_info = {k: base_info[k] for k in prompt.input_variables}\\n    return prompt.format(**document_info)\\n\\n\\n==== File 4/5 ====\\nFile path: langchain/schema/prompt_template.py\\nTag name: BasePromptTemplate\\nCode: from __future__ import annotations\\n\\nimport json\\nfrom abc import ABC, abstractmethod\\nfrom pathlib import Path\\nfrom typing import Any, Callable, Dict, List, Mapping, Optional, Union\\n\\nimport yaml\\nfrom pydantic import Field, root_validator\\n\\nfrom langchain.load.serializable import Serializable\\nfrom langchain.schema.document import Document\\nfrom langchain.schema.output_parser import BaseOutputParser\\nfrom langchain.schema.prompt import PromptValue\\n\\n\\nclass BasePromptTemplate(Serializable, ABC):\\n    \"\"\"Base class for all prompt templates, returning a prompt.\"\"\"\\n\\n    input_variables: List[str]\\n    \"\"\"A list of the names of the variables the prompt template expects.\"\"\"\\n    output_parser: Optional[BaseOutputParser] = None\\n    \"\"\"How to parse the output of calling an LLM on this formatted prompt.\"\"\"\\n    partial_variables: Mapping[str, Union[str, Callable[[], str]]] = Field(\\n        default_factory=dict\\n    )\\n\\n    @property\\n    def lc_serializable(self) -> bool:\\n        return True\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True\\n\\n    @abstractmethod\\n    def format_prompt(self, **kwargs: Any) -> PromptValue:\\n        \"\"\"Create Chat Messages.\"\"\"\\n\\n    @root_validator()\\n    def validate_variable_names(cls, values: Dict) -> Dict:\\n        \"\"\"Validate variable names do not include restricted names.\"\"\"\\n        if \"stop\" in values[\"input_variables\"]:\\n            raise ValueError(\\n                \"Cannot have an input variable named \\'stop\\', as it is used internally,\"\\n                \" please rename.\"\\n            )\\n        if \"stop\" in values[\"partial_variables\"]:\\n            raise ValueError(\\n                \"Cannot have an partial variable named \\'stop\\', as it is used \"\\n                \"internally, please rename.\"\\n            )\\n\\n        overall = set(values[\"input_variables\"]).intersection(\\n            values[\"partial_variables\"]\\n        )\\n        if overall:\\n            raise ValueError(\\n                f\"Found overlapping input and partial variables: {overall}\"\\n            )\\n        return values\\n\\n    def partial(self, **kwargs: Union[str, Callable[[], str]]) -> BasePromptTemplate:\\n        \"\"\"Return a partial of the prompt template.\"\"\"\\n        prompt_dict = self.__dict__.copy()\\n        prompt_dict[\"input_variables\"] = list(\\n            set(self.input_variables).difference(kwargs)\\n        )\\n        prompt_dict[\"partial_variables\"] = {**self.partial_variables, **kwargs}\\n        return type(self)(**prompt_dict)\\n\\n    def _merge_partial_and_user_variables(self, **kwargs: Any) -> Dict[str, Any]:\\n        # Get partial params:\\n        partial_kwargs = {\\n            k: v if isinstance(v, str) else v()\\n            for k, v in self.partial_variables.items()\\n        }\\n        return {**partial_kwargs, **kwargs}\\n\\n    @abstractmethod\\n    def format(self, **kwargs: Any) -> str:\\n        \"\"\"Format the prompt with the inputs.\\n\\n        Args:\\n            kwargs: Any arguments to be passed to the prompt template.\\n\\n        Returns:\\n            A formatted string.\\n\\n        Example:\\n\\n        .. code-block:: python\\n\\n            prompt.format(variable1=\"foo\")\\n        \"\"\"\\n\\n    @property\\n    def _prompt_type(self) -> str:\\n        \"\"\"Return the prompt type key.\"\"\"\\n        raise NotImplementedError\\n\\n    def dict(self, **kwargs: Any) -> Dict:\\n        \"\"\"Return dictionary representation of prompt.\"\"\"\\n        prompt_dict = super().dict(**kwargs)\\n        prompt_dict[\"_type\"] = self._prompt_type\\n        return prompt_dict\\n\\n    def save(self, file_path: Union[Path, str]) -> None:\\n        \"\"\"Save the prompt.\\n\\n        Args:\\n            file_path: Path to directory to save prompt to.\\n\\n        Example:\\n        .. code-block:: python\\n\\n            prompt.save(file_path=\"path/prompt.yaml\")\\n        \"\"\"\\n        if self.partial_variables:\\n            raise ValueError(\"Cannot save prompt with partial variables.\")\\n        # Convert file to Path object.\\n        if isinstance(file_path, str):\\n            save_path = Path(file_path)\\n        else:\\n            save_path = file_path\\n\\n        directory_path = save_path.parent\\n        directory_path.mkdir(parents=True, exist_ok=True)\\n\\n        # Fetch dictionary to save\\n        prompt_dict = self.dict()\\n\\n        if save_path.suffix == \".json\":\\n            with open(file_path, \"w\") as f:\\n                json.dump(prompt_dict, f, indent=4)\\n        elif save_path.suffix == \".yaml\":\\n            with open(file_path, \"w\") as f:\\n                yaml.dump(prompt_dict, f, default_flow_style=False)\\n        else:\\n            raise ValueError(f\"{save_path} must be json or yaml\")\\n\\n\\ndef format_document(doc: Document, prompt: BasePromptTemplate) -> str:\\n    \"\"\"Format a document into a string based on a prompt template.\\n\\n    First, this pulls information from the document from two sources:\\n\\n    1. `page_content`:\\n        This takes the information from the `document.page_content`\\n        and assigns it to a variable named `page_content`.\\n    2. metadata:\\n        This takes information from `document.metadata` and assigns\\n        it to variables of the same name.\\n\\n    Those variables are then passed into the `prompt` to produce a formatted string.\\n\\n    Args:\\n        doc: Document, the page_content and metadata will be used to create\\n            the final string.\\n        prompt: BasePromptTemplate, will be used to format the page_content\\n            and metadata into the final string.\\n\\n    Returns:\\n        string of the document formatted.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.schema import Document\\n            from langchain.prompts import PromptTemplate\\n            doc = Document(page_content=\"This is a joke\", metadata={\"page\": \"1\"})\\n            prompt = PromptTemplate.from_template(\"Page {page}: {page_content}\")\\n            format_document(doc, prompt)\\n            >>> \"Page 1: This is a joke\"\\n    \"\"\"\\n    base_info = {\"page_content\": doc.page_content, **doc.metadata}\\n    missing_metadata = set(prompt.input_variables).difference(base_info)\\n    if len(missing_metadata) > 0:\\n        required_metadata = [\\n            iv for iv in prompt.input_variables if iv != \"page_content\"\\n        ]\\n        raise ValueError(\\n            f\"Document prompt requires documents to have metadata variables: \"\\n            f\"{required_metadata}. Received document with missing metadata: \"\\n            f\"{list(missing_metadata)}.\"\\n        )\\n    document_info = {k: base_info[k] for k in prompt.input_variables}\\n    return prompt.format(**document_info)\\n\\n\\n==== File 5/5 ====\\nFile path: langchain/schema/prompt_template.py\\nTag name: format_prompt\\nCode: from __future__ import annotations\\n\\nimport json\\nfrom abc import ABC, abstractmethod\\nfrom pathlib import Path\\nfrom typing import Any, Callable, Dict, List, Mapping, Optional, Union\\n\\nimport yaml\\nfrom pydantic import Field, root_validator\\n\\nfrom langchain.load.serializable import Serializable\\nfrom langchain.schema.document import Document\\nfrom langchain.schema.output_parser import BaseOutputParser\\nfrom langchain.schema.prompt import PromptValue\\n\\n\\nclass BasePromptTemplate(Serializable, ABC):\\n    \"\"\"Base class for all prompt templates, returning a prompt.\"\"\"\\n\\n    input_variables: List[str]\\n    \"\"\"A list of the names of the variables the prompt template expects.\"\"\"\\n    output_parser: Optional[BaseOutputParser] = None\\n    \"\"\"How to parse the output of calling an LLM on this formatted prompt.\"\"\"\\n    partial_variables: Mapping[str, Union[str, Callable[[], str]]] = Field(\\n        default_factory=dict\\n    )\\n\\n    @property\\n    def lc_serializable(self) -> bool:\\n        return True\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True\\n\\n    @abstractmethod\\n    def format_prompt(self, **kwargs: Any) -> PromptValue:\\n        \"\"\"Create Chat Messages.\"\"\"\\n\\n    @root_validator()\\n    def validate_variable_names(cls, values: Dict) -> Dict:\\n        \"\"\"Validate variable names do not include restricted names.\"\"\"\\n        if \"stop\" in values[\"input_variables\"]:\\n            raise ValueError(\\n                \"Cannot have an input variable named \\'stop\\', as it is used internally,\"\\n                \" please rename.\"\\n            )\\n        if \"stop\" in values[\"partial_variables\"]:\\n            raise ValueError(\\n                \"Cannot have an partial variable named \\'stop\\', as it is used \"\\n                \"internally, please rename.\"\\n            )\\n\\n        overall = set(values[\"input_variables\"]).intersection(\\n            values[\"partial_variables\"]\\n        )\\n        if overall:\\n            raise ValueError(\\n                f\"Found overlapping input and partial variables: {overall}\"\\n            )\\n        return values\\n\\n    def partial(self, **kwargs: Union[str, Callable[[], str]]) -> BasePromptTemplate:\\n        \"\"\"Return a partial of the prompt template.\"\"\"\\n        prompt_dict = self.__dict__.copy()\\n        prompt_dict[\"input_variables\"] = list(\\n            set(self.input_variables).difference(kwargs)\\n        )\\n        prompt_dict[\"partial_variables\"] = {**self.partial_variables, **kwargs}\\n        return type(self)(**prompt_dict)\\n\\n    def _merge_partial_and_user_variables(self, **kwargs: Any) -> Dict[str, Any]:\\n        # Get partial params:\\n        partial_kwargs = {\\n            k: v if isinstance(v, str) else v()\\n            for k, v in self.partial_variables.items()\\n        }\\n        return {**partial_kwargs, **kwargs}\\n\\n    @abstractmethod\\n    def format(self, **kwargs: Any) -> str:\\n        \"\"\"Format the prompt with the inputs.\\n\\n        Args:\\n            kwargs: Any arguments to be passed to the prompt template.\\n\\n        Returns:\\n            A formatted string.\\n\\n        Example:\\n\\n        .. code-block:: python\\n\\n            prompt.format(variable1=\"foo\")\\n        \"\"\"\\n\\n    @property\\n    def _prompt_type(self) -> str:\\n        \"\"\"Return the prompt type key.\"\"\"\\n        raise NotImplementedError\\n\\n    def dict(self, **kwargs: Any) -> Dict:\\n        \"\"\"Return dictionary representation of prompt.\"\"\"\\n        prompt_dict = super().dict(**kwargs)\\n        prompt_dict[\"_type\"] = self._prompt_type\\n        return prompt_dict\\n\\n    def save(self, file_path: Union[Path, str]) -> None:\\n        \"\"\"Save the prompt.\\n\\n        Args:\\n            file_path: Path to directory to save prompt to.\\n\\n        Example:\\n        .. code-block:: python\\n\\n            prompt.save(file_path=\"path/prompt.yaml\")\\n        \"\"\"\\n        if self.partial_variables:\\n            raise ValueError(\"Cannot save prompt with partial variables.\")\\n        # Convert file to Path object.\\n        if isinstance(file_path, str):\\n            save_path = Path(file_path)\\n        else:\\n            save_path = file_path\\n\\n        directory_path = save_path.parent\\n        directory_path.mkdir(parents=True, exist_ok=True)\\n\\n        # Fetch dictionary to save\\n        prompt_dict = self.dict()\\n\\n        if save_path.suffix == \".json\":\\n            with open(file_path, \"w\") as f:\\n                json.dump(prompt_dict, f, indent=4)\\n        elif save_path.suffix == \".yaml\":\\n            with open(file_path, \"w\") as f:\\n                yaml.dump(prompt_dict, f, default_flow_style=False)\\n        else:\\n            raise ValueError(f\"{save_path} must be json or yaml\")\\n\\n\\ndef format_document(doc: Document, prompt: BasePromptTemplate) -> str:\\n    \"\"\"Format a document into a string based on a prompt template.\\n\\n    First, this pulls information from the document from two sources:\\n\\n    1. `page_content`:\\n        This takes the information from the `document.page_content`\\n        and assigns it to a variable named `page_content`.\\n    2. metadata:\\n        This takes information from `document.metadata` and assigns\\n        it to variables of the same name.\\n\\n    Those variables are then passed into the `prompt` to produce a formatted string.\\n\\n    Args:\\n        doc: Document, the page_content and metadata will be used to create\\n            the final string.\\n        prompt: BasePromptTemplate, will be used to format the page_content\\n            and metadata into the final string.\\n\\n    Returns:\\n        string of the document formatted.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.schema import Document\\n            from langchain.prompts import PromptTemplate\\n            doc = Document(page_content=\"This is a joke\", metadata={\"page\": \"1\"})\\n            prompt = PromptTemplate.from_template(\"Page {page}: {page_content}\")\\n            format_document(doc, prompt)\\n            >>> \"Page 1: This is a joke\"\\n    \"\"\"\\n    base_info = {\"page_content\": doc.page_content, **doc.metadata}\\n    missing_metadata = set(prompt.input_variables).difference(base_info)\\n    if len(missing_metadata) > 0:\\n        required_metadata = [\\n            iv for iv in prompt.input_variables if iv != \"page_content\"\\n        ]\\n        raise ValueError(\\n            f\"Document prompt requires documents to have metadata variables: \"\\n            f\"{required_metadata}. Received document with missing metadata: \"\\n            f\"{list(missing_metadata)}.\"\\n        )\\n    document_info = {k: base_info[k] for k in prompt.input_variables}\\n    return prompt.format(**document_info)\\n\\n\\n\\n\\nUser: How can i add a Custom Prompt Template in this repository?\\nAi:'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"How can i add a Custom Prompt Template in this repository?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
