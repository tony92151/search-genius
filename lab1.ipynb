{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_TYPE\", None) is not None, \"Please set your OPENAI_API_TYPE environment variable\"\n",
    "assert os.getenv(\"OPENAI_API_VERSION\", None) is not None, \"Please set your OPENAI_API_VERSION environment variable\"\n",
    "assert os.getenv(\"OPENAI_API_BASE\", None) is not None, \"Please set your OPENAI_API_BASE environment variable\"\n",
    "assert os.getenv(\"OPENAI_API_KEY\", None) is not None, \"Please set your OPENAI_API_KEY environment variable\"\n",
    "\n",
    "assert os.path.isfile('./repo/langchain/libs/langchain/tags'), \"Please run `zsh download_example_rpo.sh` first\"\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "\n",
    "# Uncomment the following line if you need to initialize FAISS with no AVX2 optimization\n",
    "# os.environ['FAISS_NO_AVX2'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tags_file(file_path):\n",
    "    with open(file_path, 'r', errors='ignore') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    tags = []\n",
    "    for line in lines:\n",
    "        if line.startswith('!'):  # Skip metadata lines\n",
    "            continue\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) >= 4:\n",
    "            tag_name = parts[0]\n",
    "            file_name = parts[1]\n",
    "            pattern = parts[2]\n",
    "            tags.append(dict(tag_name=tag_name, file_name=file_name, pattern=pattern))\n",
    "\n",
    "    return tags\n",
    "\n",
    "# Use the function\n",
    "tags = read_tags_file('./repo/langchain/libs/langchain/tags')\n",
    "\n",
    "documents = []\n",
    "\n",
    "for tag in tags:\n",
    "    documents.append(Document(page_content=f\"{tag['file_name']} | {tag['tag_name']} \", metadata=tag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use HuggingFaceEmbeddings as embedding model, this will runnuing faster in POC\n",
    "# The performance is similar to OpenAIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", model_kwargs={\"device\": \"cpu\"})\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", chunk_size = 1)\n",
    "\n",
    "# https://openai.com/blog/introducing-text-and-code-embeddings\n",
    "# embeddings = OpenAIEmbeddings(model=\"code-search-ada-code-001\", chunk_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(\"./db/faiss\"):\n",
    "    db = FAISS.load_local(folder_path=\"./db/faiss\", embeddings=embeddings, index_name=\"poc\")\n",
    "else:\n",
    "    db = FAISS.from_documents(documents, embeddings)\n",
    "    db.save_local(folder_path=\"./db/faiss\", index_name=\"poc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can i add a Custom Prompt Template in this repository?\"\n",
    "docs = db.similarity_search(query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='langchain/prompts/prompt.py | from_template ', metadata={'tag_name': 'from_template', 'file_name': 'langchain/prompts/prompt.py', 'pattern': '/^    def from_template($/;\"'}),\n",
       " Document(page_content='langchain/prompts/prompt.py | PromptTemplate ', metadata={'tag_name': 'PromptTemplate', 'file_name': 'langchain/prompts/prompt.py', 'pattern': '/^class PromptTemplate(StringPromptTemplate):$/;\"'}),\n",
       " Document(page_content='langchain/schema/prompt_template.py | _prompt_type ', metadata={'tag_name': '_prompt_type', 'file_name': 'langchain/schema/prompt_template.py', 'pattern': '/^    def _prompt_type(self) -> str:$/;\"'}),\n",
       " Document(page_content='langchain/schema/prompt_template.py | BasePromptTemplate ', metadata={'tag_name': 'BasePromptTemplate', 'file_name': 'langchain/schema/prompt_template.py', 'pattern': '/^class BasePromptTemplate(RunnableSerializable[Dict, PromptValue], ABC):$/;\"'}),\n",
       " Document(page_content='langchain/prompts/prompt.py | template_format ', metadata={'tag_name': 'template_format', 'file_name': 'langchain/prompts/prompt.py', 'pattern': '/^    template_format: Union[Literal[\"f-string\"], Literal[\"jinja2\"]] = \"f-string\"$/;\"'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import inspect\n",
    "import importlib.util\n",
    "import os\n",
    "\n",
    "root = \"repo/langchain/libs/langchain\"\n",
    " \n",
    "\n",
    "def get_source_code(function_name, function_path):\n",
    "    spec=importlib.util.spec_from_file_location(function_name, os.path.join(root, function_path))\n",
    "    foo = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(foo)\n",
    "    return inspect.getsource(foo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a hupful bot that fuilfill the human' program task:\n",
    "\n",
    "The following is releative code:\n",
    "{code_file_text}\n",
    "\n",
    "User: {user_prompt}\n",
    "Ai:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def create_code_file_text(docs : List[Document]):\n",
    "    code_file_text = \"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        code_file_text += f'==== File {i+1}/{len(docs)} ====\\n'\n",
    "        code_file_text += f'File path: {doc.metadata[\"file_name\"]}\\n'\n",
    "        code_file_text += f'Tag name: {doc.metadata[\"tag_name\"]}\\n'\n",
    "        code_file_text += f'Code: {get_source_code(doc.metadata[\"tag_name\"], doc.metadata[\"file_name\"])}\\n'\n",
    "        code_file_text += \"\\n\"\n",
    "    return code_file_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_text = create_code_file_text(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt35-chat\",\n",
    "    temperature=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(user_prompt: str) -> AIMessage:\n",
    "    # find docs similar to user_prompt\n",
    "    docs = db.similarity_search(query, k=4)\n",
    "    user_prompt = template.format(code_file_text=create_code_file_text(docs), user_prompt=user_prompt)\n",
    "\n",
    "    # call openai api here\n",
    "    message = HumanMessage(content=user_prompt)\n",
    "    return llm([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "👩‍💻 : What is Langchain design for?\n",
      "====================\n",
      "🤖 : LangChain is designed to provide a platform for building and deploying natural language processing (NLP) models and applications. It aims to make NLP more accessible and user-friendly for developers and businesses.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What is Langchain design for?\"\n",
    "result = ask(user_question).content\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(f\"👩‍💻 : {user_question}\")\n",
    "print(\"=\"*20)\n",
    "print(f\"🤖 : {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "👩‍💻 : How can i add a Custom Prompt Template in this repository? also add the unit-test. Give me an example\n",
      "====================\n",
      "🤖 : To add a custom prompt template, you can create a new class that inherits from `BasePromptTemplate` and implement the `format_prompt` method to return the desired prompt. Here's an example:\n",
      "\n",
      "```\n",
      "from langchain.schema.prompt_template import BasePromptTemplate\n",
      "\n",
      "class MyPromptTemplate(BasePromptTemplate):\n",
      "    def format_prompt(self, **kwargs):\n",
      "        return f\"My custom prompt with {kwargs['variable']}\"\n",
      "\n",
      "# Unit test\n",
      "def test_my_prompt_template():\n",
      "    prompt = MyPromptTemplate(input_variables=[\"variable\"])\n",
      "    output = prompt.format_prompt(variable=\"test\")\n",
      "    assert output == \"My custom prompt with test\"\n",
      "```\n",
      "\n",
      "You can then use this new prompt template in your LangChain project by importing it and instantiating it with the desired input variables.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"How can i add a Custom Prompt Template in this repository? also add the unit-test. Give me an example\"\n",
    "result = ask(user_question).content\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(f\"👩‍💻 : {user_question}\")\n",
    "print(\"=\"*20)\n",
    "print(f\"🤖 : {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "👩‍💻 : What is the high-level system architecture of this project? Give me an example\n",
      "====================\n",
      "🤖 : The high-level system architecture of this project consists of various components such as prompts, models, and output parsers. A prompt is a template that accepts a set of parameters from the user that can be used to generate a prompt for a language model. The model is then used to generate an output based on the prompt. Finally, the output parser is used to parse the output and return a structured response. \n",
      "\n",
      "For example, let's say we have a prompt that asks for a summary of a news article. The user provides the article title and content as parameters to the prompt. The model then generates a summary of the article based on the provided parameters. Finally, the output parser is used to extract the summary and return it as a structured response.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What is the high-level system architecture of this project? Give me an example\"\n",
    "result = ask(user_question).content\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(f\"👩‍💻 : {user_question}\")\n",
    "print(\"=\"*20)\n",
    "print(f\"🤖 : {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of GPT-3.5 not knowing the langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I do not have any information about a term or concept called \"langchain.\" Could you please provide more context or details about what you are referring to?\n"
     ]
    }
   ],
   "source": [
    "print(llm([HumanMessage(content=\"What is langchain\")]).content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
