{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_TYPE\", None) is not None, \"Please set your OPENAI_API_TYPE environment variable\"\n",
    "assert os.getenv(\"OPENAI_API_VERSION\", None) is not None, \"Please set your OPENAI_API_VERSION environment variable\"\n",
    "assert os.getenv(\"OPENAI_API_BASE\", None) is not None, \"Please set your OPENAI_API_BASE environment variable\"\n",
    "assert os.getenv(\"OPENAI_API_KEY\", None) is not None, \"Please set your OPENAI_API_KEY environment variable\"\n",
    "\n",
    "assert os.path.isfile('./repo/langchain/libs/langchain/tags'), \"Please run `zsh download_example_rpo.sh` first\"\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "\n",
    "# Uncomment the following line if you need to initialize FAISS with no AVX2 optimization\n",
    "# os.environ['FAISS_NO_AVX2'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tags_file(file_path):\n",
    "    with open(file_path, 'r', errors='ignore') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    tags = []\n",
    "    for line in lines:\n",
    "        if line.startswith('!'):  # Skip metadata lines\n",
    "            continue\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) >= 4:\n",
    "            tag_name = parts[0]\n",
    "            file_name = parts[1]\n",
    "            pattern = parts[2]\n",
    "            tags.append(dict(tag_name=tag_name, file_name=file_name, pattern=pattern))\n",
    "\n",
    "    return tags\n",
    "\n",
    "# Use the function\n",
    "tags = read_tags_file('./repo/langchain/libs/langchain/tags')\n",
    "\n",
    "documents = []\n",
    "\n",
    "for tag in tags:\n",
    "    documents.append(Document(page_content=f\"{tag['file_name']} | {tag['tag_name']} \", metadata=tag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use HuggingFaceEmbeddings as embedding model, this will runnuing faster in POC\n",
    "# The performance is similar to OpenAIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", model_kwargs={\"device\": \"cpu\"})\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", chunk_size = 1)\n",
    "\n",
    "# https://openai.com/blog/introducing-text-and-code-embeddings\n",
    "# embeddings = OpenAIEmbeddings(model=\"code-search-ada-code-001\", chunk_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(\"./db/faiss\"):\n",
    "    db = FAISS.load_local(folder_path=\"./db/faiss\", embeddings=embeddings, index_name=\"poc\")\n",
    "else:\n",
    "    db = FAISS.from_documents(documents, embeddings)\n",
    "    db.save_local(folder_path=\"./db/faiss\", index_name=\"poc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can i add a Custom Prompt Template in this repository?\"\n",
    "docs = db.similarity_search(query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='langchain/prompts/prompt.py | from_template ', metadata={'tag_name': 'from_template', 'file_name': 'langchain/prompts/prompt.py', 'pattern': '/^    def from_template(cls, template: str, **kwargs: Any) -> PromptTemplate:$/;\"'}),\n",
       " Document(page_content='langchain/prompts/prompt.py | PromptTemplate ', metadata={'tag_name': 'PromptTemplate', 'file_name': 'langchain/prompts/prompt.py', 'pattern': '/^class PromptTemplate(StringPromptTemplate):$/;\"'}),\n",
       " Document(page_content='langchain/schema/prompt_template.py | _prompt_type ', metadata={'tag_name': '_prompt_type', 'file_name': 'langchain/schema/prompt_template.py', 'pattern': '/^    def _prompt_type(self) -> str:$/;\"'}),\n",
       " Document(page_content='langchain/schema/prompt_template.py | BasePromptTemplate ', metadata={'tag_name': 'BasePromptTemplate', 'file_name': 'langchain/schema/prompt_template.py', 'pattern': '/^class BasePromptTemplate(Serializable, ABC):$/;\"'}),\n",
       " Document(page_content='langchain/prompts/prompt.py | template_format ', metadata={'tag_name': 'template_format', 'file_name': 'langchain/prompts/prompt.py', 'pattern': '/^    template_format: str = \"f-string\"$/;\"'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import inspect\n",
    "import importlib.util\n",
    "import os\n",
    "\n",
    "root = \"repo/langchain/libs/langchain\"\n",
    " \n",
    "\n",
    "def get_source_code(function_name, function_path):\n",
    "    spec=importlib.util.spec_from_file_location(function_name, os.path.join(root, function_path))\n",
    "    foo = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(foo)\n",
    "    return inspect.getsource(foo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a hupful bot that fuilfill the human' program task:\n",
    "\n",
    "The following is releative code:\n",
    "{code_file_text}\n",
    "\n",
    "User: {user_prompt}\n",
    "Ai:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def create_code_file_text(docs : List[Document]):\n",
    "    code_file_text = \"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        code_file_text += f'==== File {i+1}/{len(docs)} ====\\n'\n",
    "        code_file_text += f'File path: {doc.metadata[\"file_name\"]}\\n'\n",
    "        code_file_text += f'Tag name: {doc.metadata[\"tag_name\"]}\\n'\n",
    "        code_file_text += f'Code: {get_source_code(doc.metadata[\"tag_name\"], doc.metadata[\"file_name\"])}\\n'\n",
    "        code_file_text += \"\\n\"\n",
    "    return code_file_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_text = create_code_file_text(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt35-chat\",\n",
    "    temperature=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(user_prompt: str) -> AIMessage:\n",
    "    # find docs similar to user_prompt\n",
    "    docs = db.similarity_search(query, k=5)\n",
    "    user_prompt = template.format(code_file_text=create_code_file_text(docs), user_prompt=user_prompt)\n",
    "\n",
    "    # call openai api here\n",
    "    message = HumanMessage(content=user_prompt)\n",
    "    return llm([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "👩‍💻 : What is Langchain design for?\n",
      "====================\n",
      "🤖 : Langchain is a natural language processing platform designed to help users create and manage language models for a variety of applications, including chatbots, language translation, and text generation.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What is Langchain design for?\"\n",
    "result = ask(user_question).content\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(f\"👩‍💻 : {user_question}\")\n",
    "print(\"=\"*20)\n",
    "print(f\"🤖 : {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "👩‍💻 : How can i add a Custom Prompt Template in this repository? also add the unit-test. Give me an example\n",
      "====================\n",
      "🤖 : To add a custom prompt template, you can create a new file in the `langchain/prompts` directory with your template code. Here's an example:\n",
      "\n",
      "```\n",
      "# File path: langchain/prompts/custom_template.py\n",
      "\n",
      "from langchain.prompts.base import BasePromptTemplate\n",
      "\n",
      "class CustomPromptTemplate(BasePromptTemplate):\n",
      "    input_variables: List[str]\n",
      "    template: str\n",
      "\n",
      "    def format_prompt(self, **kwargs: Any) -> PromptValue:\n",
      "        # Your code here\n",
      "        pass\n",
      "\n",
      "    def format(self, **kwargs: Any) -> str:\n",
      "        # Your code here\n",
      "        pass\n",
      "\n",
      "    @property\n",
      "    def _prompt_type(self) -> str:\n",
      "        return \"custom_prompt\"\n",
      "```\n",
      "\n",
      "You can then use this custom prompt template in your code by importing it like any other module:\n",
      "\n",
      "```\n",
      "from langchain.prompts.custom_template import CustomPromptTemplate\n",
      "\n",
      "custom_prompt = CustomPromptTemplate(input_variables=[\"foo\"], template=\"Say {foo}\")\n",
      "```\n",
      "\n",
      "To add a unit test, you can create a new file in the `tests/prompts` directory with your test code. Here's an example:\n",
      "\n",
      "```\n",
      "# File path: tests/prompts/test_custom_template.py\n",
      "\n",
      "from langchain.prompts.custom_template import CustomPromptTemplate\n",
      "\n",
      "def test_custom_prompt_template():\n",
      "    prompt = CustomPromptTemplate(input_variables=[\"foo\"], template=\"Say {foo}\")\n",
      "    assert prompt.format(foo=\"hello\") == \"Say hello\"\n",
      "```\n",
      "\n",
      "You can then run the unit test using your preferred testing framework.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"How can i add a Custom Prompt Template in this repository? also add the unit-test. Give me an example\"\n",
    "result = ask(user_question).content\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(f\"👩‍💻 : {user_question}\")\n",
    "print(\"=\"*20)\n",
    "print(f\"🤖 : {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "👩‍💻 : What is the high-level system architecture of this project? Give me an example\n",
      "====================\n",
      "🤖 : The high-level system architecture of this project involves several components, including prompts, LLMs, and output parsers. Prompts define the input format for the LLMs, while output parsers define how the LLMs' output is processed. An example of this architecture in action would be using a PromptTemplate to define a prompt for a language model, passing that prompt to an LLM, and then using an output parser to extract the relevant information from the LLM's output.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What is the high-level system architecture of this project? Give me an example\"\n",
    "result = ask(user_question).content\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(f\"👩‍💻 : {user_question}\")\n",
    "print(\"=\"*20)\n",
    "print(f\"🤖 : {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of GPT-3.5 not knowing the langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I am not aware of any specific term or concept called \"langchain\". Can you please provide more context or information about what you are referring to?\n"
     ]
    }
   ],
   "source": [
    "print(llm([HumanMessage(content=\"What is langchain\")]).content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
