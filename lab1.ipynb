{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_TYPE\", None) is not None, \"Please set your OPENAI_API_TYPE environment variable\"\n",
    "assert os.getenv(\"OPENAI_API_VERSION\", None) is not None, \"Please set your OPENAI_API_VERSION environment variable\"\n",
    "assert os.getenv(\"OPENAI_API_BASE\", None) is not None, \"Please set your OPENAI_API_BASE environment variable\"\n",
    "assert os.getenv(\"OPENAI_API_KEY\", None) is not None, \"Please set your OPENAI_API_KEY environment variable\"\n",
    "\n",
    "assert os.path.isfile('./repo/langchain/libs/langchain/tags'), \"Please run `zsh download_example_rpo.sh` first\"\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "\n",
    "# Uncomment the following line if you need to initialize FAISS with no AVX2 optimization\n",
    "# os.environ['FAISS_NO_AVX2'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tags_file(file_path):\n",
    "    with open(file_path, 'r', errors='ignore') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    tags = []\n",
    "    for line in lines:\n",
    "        if line.startswith('!'):  # Skip metadata lines\n",
    "            continue\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) >= 4:\n",
    "            tag_name = parts[0]\n",
    "            file_name = parts[1]\n",
    "            pattern = parts[2]\n",
    "            tags.append(dict(tag_name=tag_name, file_name=file_name, pattern=pattern))\n",
    "\n",
    "    return tags\n",
    "\n",
    "# Use the function\n",
    "tags = read_tags_file('./repo/langchain/libs/langchain/tags')\n",
    "\n",
    "documents = []\n",
    "\n",
    "for tag in tags:\n",
    "    documents.append(Document(page_content=f\"{tag['file_name']} | {tag['tag_name']} \", metadata=tag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/private_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# You can use HuggingFaceEmbeddings as embedding model, this will runnuing faster in POC\n",
    "# The performance is similar to OpenAIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", model_kwargs={\"device\": \"cpu\"})\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", chunk_size = 1)\n",
    "\n",
    "# https://openai.com/blog/introducing-text-and-code-embeddings\n",
    "# embeddings = OpenAIEmbeddings(model=\"code-search-ada-code-001\", chunk_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(\"./db/faiss\"):\n",
    "    db = FAISS.load_local(folder_path=\"./db/faiss\", embeddings=embeddings, index_name=\"poc\")\n",
    "else:\n",
    "    db = FAISS.from_documents(documents, embeddings)\n",
    "    db.save_local(folder_path=\"./db/faiss\", index_name=\"poc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can i add a Custom Prompt Template in this repository?\"\n",
    "docs = db.similarity_search(query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='langchain/prompts/prompt.py | from_template ', metadata={'tag_name': 'from_template', 'file_name': 'langchain/prompts/prompt.py', 'pattern': '/^    def from_template(cls, template: str, **kwargs: Any) -> PromptTemplate:$/;\"'}),\n",
       " Document(page_content='langchain/prompts/prompt.py | PromptTemplate ', metadata={'tag_name': 'PromptTemplate', 'file_name': 'langchain/prompts/prompt.py', 'pattern': '/^class PromptTemplate(StringPromptTemplate):$/;\"'}),\n",
       " Document(page_content='langchain/schema/prompt_template.py | _prompt_type ', metadata={'tag_name': '_prompt_type', 'file_name': 'langchain/schema/prompt_template.py', 'pattern': '/^    def _prompt_type(self) -> str:$/;\"'}),\n",
       " Document(page_content='langchain/schema/prompt_template.py | BasePromptTemplate ', metadata={'tag_name': 'BasePromptTemplate', 'file_name': 'langchain/schema/prompt_template.py', 'pattern': '/^class BasePromptTemplate(Serializable, ABC):$/;\"'}),\n",
       " Document(page_content='langchain/prompts/prompt.py | template_format ', metadata={'tag_name': 'template_format', 'file_name': 'langchain/prompts/prompt.py', 'pattern': '/^    template_format: str = \"f-string\"$/;\"'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import inspect\n",
    "import importlib.util\n",
    "import os\n",
    "\n",
    "root = \"repo/langchain/libs/langchain\"\n",
    " \n",
    "\n",
    "def get_source_code(function_name, function_path):\n",
    "    spec=importlib.util.spec_from_file_location(function_name, os.path.join(root, function_path))\n",
    "    foo = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(foo)\n",
    "    return inspect.getsource(foo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a hupful bot that fuilfill the human' program task:\n",
    "\n",
    "The following is releative code:\n",
    "{code_file_text}\n",
    "\n",
    "User: {user_prompt}\n",
    "Ai:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def create_code_file_text(docs : List[Document]):\n",
    "    code_file_text = \"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        code_file_text += f'==== File {i+1}/{len(docs)} ====\\n'\n",
    "        code_file_text += f'File path: {doc.metadata[\"file_name\"]}\\n'\n",
    "        code_file_text += f'Tag name: {doc.metadata[\"tag_name\"]}\\n'\n",
    "        code_file_text += f'Code: {get_source_code(doc.metadata[\"tag_name\"], doc.metadata[\"file_name\"])}\\n'\n",
    "        code_file_text += \"\\n\"\n",
    "    return code_file_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_text = create_code_file_text(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt35-chat\",\n",
    "    temperature=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(user_prompt: str) -> AIMessage:\n",
    "    # find docs similar to user_prompt\n",
    "    docs = db.similarity_search(query, k=5)\n",
    "    user_prompt = template.format(code_file_text=create_code_file_text(docs), user_prompt=user_prompt)\n",
    "\n",
    "    # call openai api here\n",
    "    message = HumanMessage(content=user_prompt)\n",
    "    return llm([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"How can i add a Custom Prompt Template in this repository? Give me an example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ask(user_question).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "👩‍💻 : How can i add a Custom Prompt Template in this repository? Give me an example\n",
      "====================\n",
      "🤖 : To add a custom prompt template, you can create a new file in the `langchain/prompts` directory with the following code:\n",
      "\n",
      "```\n",
      "from langchain.prompts.base import BasePromptTemplate\n",
      "\n",
      "class CustomPromptTemplate(BasePromptTemplate):\n",
      "    input_variables: List[str]\n",
      "    template: str\n",
      "    # Add any additional properties or methods here as needed\n",
      "\n",
      "    @property\n",
      "    def _prompt_type(self) -> str:\n",
      "        return \"custom_prompt\"\n",
      "\n",
      "```\n",
      "\n",
      "Then, you can use this custom prompt template in your code by importing it and instantiating it with the desired input variables and template string:\n",
      "\n",
      "```\n",
      "from langchain.prompts import CustomPromptTemplate\n",
      "\n",
      "custom_prompt = CustomPromptTemplate(input_variables=[\"foo\"], template=\"Say {foo}\")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*20)\n",
    "print(f\"👩‍💻 : {user_question}\")\n",
    "print(\"=\"*20)\n",
    "print(f\"🤖 : {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
